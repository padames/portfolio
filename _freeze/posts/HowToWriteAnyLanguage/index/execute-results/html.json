{
  "hash": "237cf532607e5a11c02ba8966b91d9a3",
  "result": {
    "markdown": "---\ntitle: \"How to process any text with Unicode\"\nauthor: \"Pablo Adames\"\ndate: \"2025-01-21\"\ncategories: [text, C++, unicode, Rust, R]\nbibliography: references.bib\ncsl: elsevier-vancouver.csl\nformat: html\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\"\n---\n\n\n## Text is everywhere\n\nChat bots, web pages, and screen monitors display information on cell phones, home and office computers, airports, bus and train stations. The text used in these scenarios enables clear communication. Text is not impeded by sound or obstructed by environmental noise and can deliver impact and meaning through semantically dense words reinforced by visual or aural context.\n\nText and human language are technologies developed to build and transmit culture. As such, they have a fundamental role in sustaining our civilization.\n\n![Image credit: https://neuroflash.com/blog/automatic-text-editor-online/](https://neuroflash.com/wp-content/uploads/2022/09/feature-image-automatic-text-editor-online.jpg)\n\n## What is text anyway?\n\nTo narrow the scope of this post to a manageable size, let's constrain the answer to the world of computer languages. These are used to tell computers what to do when they receive data.\n\nComputer programming starts by writing instructions in a text editor. The characters used to form those instructions were initially called the ASCII characters. The characters in ASCII conveniently reflected the characters of the English alphabet plus the Arabic numerals and exclamation and punctuation marks. Thus knowing the English alphabet plus some additional computer language syntax can lower the barrier of entrance to programming a computer.\n\n## How do we use text as code?\n\nThe human-readable text instructions of a computer program have to be interpreted and then transformed into machine-readable code before the computer can execute them. The first step implies parsing the text into functional parts: line breaks, plain data, commands, operators and variable and function names.\n\nThe next step is to put those components into and abstract syntax tree (AST) that represents the program. The AST is then further converted into bite code instructions for a run time or into machine instructions to be executed in a process under the supervision of the operating system scheduler, or even directly in the microchip of a single board computer in the case of embedded systems.\n\nEverybody benefits from using text instead of programming directly using the instructions of the operating system or the bite code commands. Somehow using ASCII makes programming a computer a skill similar to learning one of the human languages. However, bias always favours the English-speaking programmer who knows how English reads.\n\nAccepting the historical facts, let's dive into the ASCII character set. Its purpose is for programmers to encode rules that control the computer as it consumes input and produces output. We call these rules the program's logic.\n\nThe input to and output from the program we will call data. Digital computers natively process only zeros and ones. Any computer data, including text, must be transformed into a sequence of these two values.\n\n## Entering text encoding\n\nText as code is handled relatively well with a small character set. How about text as data?\n\nThis is a complex subject. The complexity comes from human communication happening in rich, dynamic, and diverse cultural contexts. Tradition, convention, identity, and art influence the glyphs and alphabets used to articulate words, sentences and ultimately, ideas in human languages. These representations can be as diverse as the human groups that produce them.\n\nHow do we address this complexity? We use text encoding that machines can understand automatically and unequivocally. The accepted encoding standard is called Unicode. I will quote directly from the Unicode standard definition #51 @uni16:emoji.\n\n> Unicode is the foundation for text in all modern software: it's how all mobile phones, desktops, and other computers represent the text of every language.\n\nThere are currently a total of 1,112,064 valid Unicode values or code points. There are scalar values represented in base-16, they range of values can be seen in Table 1 below. The first 128 code points correspond to the original ASCII.\n\nThese values appear in Unicode tables associated with the typographical or artistic rendition of the glyph they represent. However, they are not used directly in the underlying machine code for each character. The main challenge is how to write text effectively in binary. Text consists of a sequence of characters, usually stored in contiguous memory.\n\nText encoding is a mapping from the code points to some efficient way to write text in binary. A fixed bit width for each character would simplify parsing characters within some text sequence and it would make character boundaries trivial to infer, however, it would waste much space when representing the small ASCII characters. On the other side if we choose a variable bit width then space would be saved with the small values but an end-of-character marker would be required, also adding extra space and complexity.\n\nThe common Unicode text encoding to write source code in modern computer languages is called UTF-8, it is the encoding used to store the text of almost the entirety of the web pages of the Internet. UTF stands for the Unicode Transformation Format @enwiki:1268238400_. This is a super set of the ASCII character set used in the first computer languages like COBOL, FORTRAN and BASIC.\n\n\n| Range of Code Points      | Byte 1   | Byte 2   | Byte 3  | Byte 4  |\n|---------------------------|----------|----------|---------|---------|\n| From U+0000 to U007F      | 0yyyzzzz |          |         |         |\n| From U+0080 to U+07FF     | 110xxxyy | 10yyzzzz |         |         |\n| From U+0800 to U+FFFF     | 1110wwww | 10xxxxyy | 10yyzzzz|         |\n| From U+010000 to U+10FFFF | 11110uvv | 10vvwwww | 10xxxxyy| 10yyzzzz|\n\n: Table 1. Code point to UTF-8 encoding, from @convertcodes:encodeformats {.striped .hover}\n\nThe code values or code points in Table 1 have a corresponding multi-byte representation in UTF-8 form, the encoding has a variable number of bytes. How many bytes is encoded by the number of consecutive ones in the most significant digits of the first byte in a byte sequence: none means the current byte is sufficient to encode the character fully, that is, it fits in one byte, e.g. an ASCII character. One means the byte is part of a multi-byte sequence. Two means the byte marks the start of a character encoded in two bytes in total. Three and four ones mean the equivalent, the byte marks the beginning of characters represented by three and four bytes respectively, including the leading one. Table 2 shows examples of each one of these cases.\n\n\n| Character | Code point | Byte 1   | Byte 2   | Byte 3   | Byte 4   |\n|-----------|------------|----------|----------|----------|----------|\n|H          |  U+0048    | 01001000 |          |          |          |\n|√°          |  U+00E1    | 11000011 | 10100001 |          |          |\n|·úã  |      U+170B | 11100001    |    10011100 | 10001011 |          | \n|üçá         | U+01F347   | 11110000 | 10011111 | 10001101 | 10000111 |\n\n: Table 2. Examples of UTF-8 bytes for characters in each of the ranges of Unicode points presented in Table 1. {.striped .hover}\n\n\nThis means that there is no need for escape sequences to mark the boundary of a single encoded multi-byte character. It also mean there is no need of a special marker for characters boundaries in a sequence of many characters. In addition to that it isolates semantic representation from character identification. Unicode encoding takes care of character boundaries but semantic parsing takes care of instruction or operator boundary.\n\n## Trade-offs of choosing a Unicode encoding\n\nThe code points can be encoded in three forms: UTF-8, UTF-16 and UTF-32. Each encoding maps the code points to unique code unit sequences of variable length @uni16:encodingforms. In particular, the larger code points need multiple single code units if using the code form UTF-8. \nConversely they may fit into a single code unit if encoded in UTF-32. \nThis happens because the width of the smallest code unit is 8, 16, or 32 bits respectively for each of the forms.\nSee Table 3 below for a comparison of three different encodings for a text sentence using short and long code points. \n\n\n| Encoding| Hexadecimal |\n|--------|----------------------------------------------------------|\n| UTF-8  | 48 65 6c 6c 6f f0 9f 8d 87 2c 20 f0 9f 8d 88 21                         |\n| UTF-16 | 0048 0065 006c 006c 006f d83c df47 002c 0020 d83c df48 0021   |\n| UTF-32 | 00000048 00000065 0000006c 0000006c 0000006f 0001f347 0000002c 00000020 0001f348 00000021 |\n\n: Table 3. Unicode encoding of the sentence `Helloüçá, üçà!` from the website [Convert Codes](https://convertcodes.com/unicode-converter-encode-decode-utf/) {.striped .hover}\n\nThe long code points for the fruit emojis occupy a single UTF-32 code unit, look at 0001f347 and 0001f348, `üçá` and `üçà` in Table 4 below. from the same table one sees how these long codes require two code units in UTF-16 and four in UTF-8. That means more processing to read and write as more code units are required.\n\nThe selection of a encoding form to map code points to code units forces a trade off between space and complexity. UTF-8 is very efficient for handling the smaller Unicode values while it is complex to handle the large ones. Conversely, UTF-32 is wasteful for storing the smaller values but simple for the higher ones.\n\nWindows chose to use UTF-16 natively since the NT version. It is efficient for the Asian alphabets that require at least two code points in UTF-8 but one in UTF-16 for most of their characters. \n\nConsider the following emojis, their Unicode values and their multi-byte representation in three different encodings:\n\n| fruit emoji | name  | Unicode point | UTF-8 | UTF-16 | UTF-32 |\n|-------------|-------|---------------|-------|--------|--------|\n| üçá          | grapes| U+1F347 |f0 9f 8d 87| d83c udf47 | 0001f347|\n| üçà          | melon| U+1F348 |f0 9f 8d 88| d83c df48 | 0001f348 |\n| üçâ          | watermelon| U+1F349 |f0 9f 8d 89| d83c df49 | 0001f349 |\n\n: Table 4. Food-fruit emoji codes and equivalent encoded representations {.striped .hover}\n\n\nIn general, UTF-8 is the most suitable option for web pages and computer programs. The reason is only pragmatic because it makes the web engines and text editors and parsers work well when consuming most html, Javascript, and general purpose computer languages.\n\nThe Windows operating system uses APIs that handle the UTF-8 to UTF-16 conversion internally. In that way application code can pass UTF-8 encoded to the operating system.\n\n\n### More definitions\n\nWe are not done with definitions yet. Some additional considerations are necessary to process text correctly, for this a detailed reading of the Unicode standard is advised @uni16:texthandling. I will focus first on the difference between characters and glyphs.\n\nA character is an abstract representation of a concrete mark made on paper or rendered to a computer screen, a so-called glyph. The character is the Unicode value that matches a glyph by convention.\n\nThe standard defines how to represent and how to identify a text character as a code point, however it does not provide rules for determining what a valid text element is because that depends on what the context is. Examples of context are capitalization for a title in English text, or the brake down of long sequences of characters at the end of text lines.\n\nA text unit, thus is a valid sequence of one or more encoded text characters @uni16:texthandling.\n\n### Algorithms for text encoding\n\nI will spare the reader with the specifics of these algorithms to go from code point to any of the encodings. A easy to follow example of these algorithms can be found elsewhere, I like the rich examples from this website: [utf-8 encode-decode](https://convertcodes.com/utf8-encode-decode-convert-string/).\nIf you follow those examples you will realize a text processing program needs a table of code points to glyphs, and an algorithm to encode code points to code units and the reversal.\n\n\n## Practical character boundary examples\n\nI will present examples of character boundary identification in different computer languages. We will limit the scope to identifying emoticons mixed in with text for rendering text using UTF-8 encoding as most modern editors and terminals would do.\n\nI will be using emojis from the food-fruit category, as defined in the Unicode standard chart shown in the figure below.\n\n![Figure 1. The table for the food-fruit emoji category taken from the Unicode standard chart @uni16:emojicharts](Food-fruit-unicode-emoji-chart.png)\n\n\n### C++\n\nThe platform agnostic support for UTF-8 in C++ 20 is not quite as simple as in other modern computer languages. POSIX operating systems like Linux work with UTF-8 out of the box. However Windows uses UTF-16 internally, making every system call dependent on text encoding translations.\n\nOne of the alternatives is to use the Boost `nowide` libraries to make the IO UTF-8 aware regardles of the platform. The source code can be downloaded from [Boost nowide GitHub](https://github.com/boostorg/nowide) and it can be built with the CMake build system. On Ubuntu there is need to install supporting Boost libraries via: `sudo apt-get -y install libboost-filesystem-dev`.  \n\n```c++\n#include <boost/nowide/args.hpp>\n#include <boost/nowide/fstream.hpp>\n#include <boost/nowide/iostream.hpp>\n#include <vector>\n\nint main(int argc,char **argv)\n{\n    std::vector<std::string> fruit;\n    fruit.push_back(\"üçá\");\n    fruit.push_back(\"üçà\");\n    fruit.push_back(\"üçâ\");\n    fruit.push_back(\"üçä\");\n    fruit.push_back(\"üçã\");\n\n    std::string test_string = fruit[0];\n\n    std::string long_fruity_string = \"Hello\" + test_string + \", \" + fruit[1];\n\n    boost::nowide::cout << long_fruity_string << std::endl;\n\n    for ( const char& letter: long_fruity_string){\n        boost::nowide::cout << letter << std::endl;\n    }\n    return 0;\n}\n```\nProduces the following output in VSCode:\n\n```\n[Running] cd \"/home/pablo/git/CPP/character_boundaries/\" && g++ main.cpp -o main && \"/home/pablo/git/CPP/character_boundaries/\"main\nHelloüçá, üçà\nH\ne\nl\nl\no\nÔøΩ\nÔøΩ\nÔøΩ\nÔøΩ\n,\n \nÔøΩ\nÔøΩ\nÔøΩ\nÔøΩ\n```\n\nIt is non-trivial to find the character boundaries in C++. The std::string treats the emojis as 4 `u_int8_t` bytes but it does not map them to the glyphs for rendering.\n\n### Rust\n\nIn Rust I used the crate `emojis` from the public registry. A `crate` is a module in Rust. For more details read the free [cargo online book](https://doc.rust-lang.org/cargo/index.html).\n\nThe code creates a short vector with the first 5 elements of the Unicode emojis in the Food and Drink group. Then it creates a Rust String by concatenating String slices in UTF-8. It follows by using the magic of the String function `chars` to find the character boundaries and to print them one by one. The 'magic' comes from the fact that some of these characters have multiple code unit representations in UTF-8, the default this is the Unicode form used by the Rust String data type, regardless of the operating system.\n\nIn other words the function has to parse the byte sequence and using the UTF-8 decoding rules find whole characters, using the rules shown in Table 1. Once the base 16 Unicode code points are found a rendering function has to find them in a table and create the matching glyph on the terminal for printing.\n\nFinally, the binary sequence for the whole sentence is printed to the terminal.\n\n``` rust\nextern crate emojis;\n\nfn main() {\n    let fruit: Vec<_> = emojis::Group::FoodAndDrink.emojis().map(|e| e.as_str()).take(5).collect();\n    assert_eq!(fruit, [\"üçá\", \"üçà\", \"üçâ\", \"üçä\", \"üçã\"]);\n    \n    let test_string = String::from(fruit[0]);\n    let long_fruity_string = \"Hello\".to_owned() + &test_string + \", \" + fruit[1]; \n    \n    println!(\"{}!\", long_fruity_string);\n\n    for letter in long_fruity_string.chars() {\n        println!(\"{letter}\");\n    }\n    let mut long_fruity_string_in_binary = \"\".to_string();\n\n    for character in long_fruity_string.clone().into_bytes() {\n        long_fruity_string_in_binary += &format!(\"0{:b} \", character);\n    }\n    println!(r#\"\"{}\" in binary is \"{}\"\"#, long_fruity_string, long_fruity_string_in_binary);\n```\n\nThe output:\n\n```         \n$ cargo run\n   Compiling word_boundary v0.1.0 (/home/pablo/git/Rust/rust-practice/word_boundary)\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.22s\n     Running `target/debug/word_boundary`\nHelloüçá, üçà!\nH\ne\nl\nl\no\nüçá\n,\n \nüçà\n\"Helloüçá, üçà\" in binary is \"01001000 01100101 01101100 01101100 01101111 011110000 010011111 010001101 010000111 0101100 0100000 011110000 010011111 010001101 010001000 \"\n```\n\nRemember the rules for representing UTF-8 encoded bytes from standard Unicode points, well if you have a look at the last four bytes of the sentence above: `11110000 10011111 10001101 10001000` removing the initial 0 that indicates binary numbers, we observe that these correspond to the 4 bytes of the multi-byte encoded character üçà, U+1F348, in UTF-8 encoding in hexadecimal is f0 9f 8d 88 (from Table 3).\n\n### Python\n\nComing soon...\n\n### R\n\nR has the `emoji` package. After a short exploration of its manual @r-package:emoji it is possible to print the characters of a sentence using the fruit emojis by identifying their boundaries boundaries seamlessly.\n\n``` r\ninstall.packages(\"emoji\")\nlibrary(\"emoji\")\nfruit <- emojis[ emojis$name %in% c(\"grapes\", \"watermelon\", \"melon\", \"lemon\", \"tangerine\"),]$emoji\ntest_string <- fruit[1]\nlong_fruity_string <- paste(\"Hello\", test_string,\", \",fruit[2] )\npaste(long_fruity_string)\nresults <- strsplit(x = long_fruity_string, split = \"\")\n#'strsplit' returns a list in case x has multiple strings to be processed, we grab only the first one\nresults[[1]]\n```\n\nProduces the following output, a vector of characters, due to R's native focus on vectorized operations:\n\n```         \n [1] \"Hello üçá ,  üçà\"\n [1] \"H\"  \"e\"  \"l\"  \"l\"  \"o\"  \" \"  \"üçá\" \" \"  \",\"  \" \"  \" \"  \"üçà\"\n```\n\n## Conclusion\n\nText representation using Unicode values and encondings has become an invisible technical detail. \nIts standarization, maintenance, and enhancement is in the hands of a few experts, interest groups and users that drive their constant adaptation to modern communication via the web.\n\nThe software we use already handles text encoding, most likely UTF-8, via the libraries of the computer languages used to build those programs.\nWe saw examples of how well they do it. \nFrom human readable to binary data that can travel through wires, optic fibre, or electromagnetic waves, text is everywhere.\n\n\n### References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}