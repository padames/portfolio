[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Software Engineering bits",
    "section": "",
    "text": "This is the home for a few short posts and presentations made throughout the years. Most are related to subjects of personal interest like data modeling and visualization. I am also very fond of Machine Learning, High-Performance computing, and computer languages in general.\n\nEverything here is licensed under Creative Commons Attribution-ShareAlike 4.0 International\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHow to process any text with Unicode\n\n\n\n\n\n\n\ntext\n\n\nC++\n\n\nUnicode\n\n\nRust\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nPablo Adames\n\n\n\n\n\n\n  \n\n\n\n\nAnimation to explain coupled time-series\n\n\n\n\n\n\n\nVisualization\n\n\nR\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\nPablo Adames\n\n\n\n\n\n\n  \n\n\n\n\nHow to create a GIF file from individual images\n\n\n\n\n\n\n\nVisualization\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2021\n\n\nPablo Adames\n\n\n\n\n\n\n  \n\n\n\n\nA User Interface to Analyze Storm Data\n\n\n\n\n\n\n\nVisualization\n\n\nR\n\n\nLeaflet\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2017\n\n\nPablo Adames\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Animation to explain coupled time-series",
    "section": "",
    "text": "I consider Edward R. Tufte one of the modern experts in data visualization. I was able to attend one of his talks in Seattle in May, 2019. We walked away with a beautiful set of his most important books and the challenge of stuffing them in my carry-on bag on the flight back to Calgary.\nAside from the pleasant experience and the wonderful books I bring this up because there are two kinds of data visualization examples he introduces at the beginning of his book, “The Visual Display of Quantitative Information”: data maps and time series. Visual excellence, Tufte claims in his book, is achieved when one can convey an idea in the shortest time using the least amount of ink and printed space to an observer.\nOne of the time-series examples is a facet grid plot representing the evolution of unemployment rate and inflation in several countries. The term facet grid, of course is adopted from ggplot lingo."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a personal blog with entries reflecting my professional interests in computer languages and data analysis and visualization.\nMost of my professional experience has been developing Engineering applications using Matlab for modelling and C++ for library implementation. I like Python in spite of relying on indentation alone for scoping blocks of code. Strongly typed and more declarative than procedural languages win the day for me. I have way too many books on Haskell, a language that has formed me in many ways. I love functional programming and I am so glad C++ and Python have embraced it.\nIn the last 5 years, after completing a Master of Engineering in Software Engineering at the University of Calgary I have had the chance to designing and use relational and document (also called noSQL) databases for system development.\nI am pretty good at Linux bash programming as some of my colleagues at work know too well. However I am fonder of more Lispy languages like R when it comes to scripting. As far as natively compiled languages I recently I took up Rust after realizing how much of an improvement it is over C and C++ in terms of memory safety and overall speed of development.\nI will write more on this Rust journey because I think this is technology that can change the way we build software infrastructure."
  },
  {
    "objectID": "posts/welcome/index.html#the-problem-statement",
    "href": "posts/welcome/index.html#the-problem-statement",
    "title": "Animation to explain coupled time-series",
    "section": "The problem statement",
    "text": "The problem statement\nOne of the examples used in Hadley Wickham’s book, “ggplot2 - Elegant Grahics for Data Analysis”, has also to do with the representation of time evolution of two economic variables related to unemployment in the USA. The current computational resources available to us have made it relatively easy to address these visualization challenges to achieve visual excellence.\nThis blog post is about the way we can add a third dimension to a 2-D plot using animation to explore the evolution of two related time-series.\nIndividual time dependent variables are easily visualized by drawing their time series representation on a two-dimensional plot. The difficulty becomes visualizing two time series in the same traditional graph. One elegant solution was mentioned in Tufte’s book mentioned in the introduction to this post. Let’s use animation to add more insight into the data."
  },
  {
    "objectID": "posts/welcome/index.html#the-economics-data-set",
    "href": "posts/welcome/index.html#the-economics-data-set",
    "title": "Animation to explain coupled time-series",
    "section": "The economics data set",
    "text": "The economics data set\nThese examples are re-used from section 2.6.5 of https://ggplot2-book.org/getting-started#sec-line.\nThe data set called economics from the ggplot2 package, has employment statistics on the US measured over the last 40 years up until 2015.\nHere is a brief look at the first 5 out of 574 rows of the dataframe economics.\n\ndata &lt;- head(economics, n=5)\nknitr::kable(data)\n\n\n\n\ndate\npce\npop\npsavert\nuempmed\nunemploy\n\n\n\n\n1967-07-01\n506.7\n198712\n12.6\n4.5\n2944\n\n\n1967-08-01\n509.8\n198911\n12.6\n4.7\n2945\n\n\n1967-09-01\n515.6\n199113\n11.9\n4.6\n2958\n\n\n1967-10-01\n512.2\n199311\n12.9\n4.9\n3143\n\n\n1967-11-01\n517.4\n199498\n12.8\n4.7\n3066"
  },
  {
    "objectID": "posts/welcome/index.html#visualizing-the-unemployment-rate",
    "href": "posts/welcome/index.html#visualizing-the-unemployment-rate",
    "title": "Animation to explain coupled time-series",
    "section": "Visualizing the Unemployment Rate",
    "text": "Visualizing the Unemployment Rate\nLet’s first make a simple time series plot of the unemployment rate. This is a continuous variable that is computed with the ratio unemploy/pop.\nIn ggplot2 a frame defines the first mapping from variables to a space where the data will be represented. It is created with the function aes(). The obvious frame for this plot is defined by the two variables date and unemploy / pop. They are mapped to the x and y coordinates of a 2-D plane. The glyphs drawn over this frame will be lines between the data points located in the frame, they are created with the function geom_line(). This function defines a layer over the frame.\n\nggplot(data = economics, mapping = aes(x = date, y = unemploy / pop)) +\n  geom_line()\n\n\n\n\nTechnically speaking unemploy / pop represents the “population rate of unemployment as a fraction of the population able to work that is unemployed”, (https://www.bls.gov/cps/cps_htgm.htm#definitions)"
  },
  {
    "objectID": "posts/welcome/index.html#visualizing-the-unemployment-median-duration-in-weeks",
    "href": "posts/welcome/index.html#visualizing-the-unemployment-median-duration-in-weeks",
    "title": "Animation to explain coupled time-series",
    "section": "Visualizing the unemployment median duration in weeks",
    "text": "Visualizing the unemployment median duration in weeks\nAnother variable called uempmed from the same dataset tracks the median length of time in weeks of unemployment.\n\nggplot(economics, aes(date, uempmed)) +\n  geom_line()\n\n\n\n\nFrom these two plots one can observe the recent trend towards longer median unemployment time in the decade of 2010. There are also cycles of between 5 and 10 years of peak unemployment rates.\nAn interesting question is how these two time series correlate over time. Are there interactions between these two variables that we could observe in one plot?"
  },
  {
    "objectID": "posts/welcome/index.html#visualizing-both-variables-in-the-same-plot",
    "href": "posts/welcome/index.html#visualizing-both-variables-in-the-same-plot",
    "title": "Animation to explain coupled time-series",
    "section": "Visualizing both variables in the same plot",
    "text": "Visualizing both variables in the same plot\nIn ggplot2, the frame for a representation that shows both variables on an line plot can be defined by a mapping of each variable to the x and y coordinates of the plane. We can create two types of glyphs over it: one is points shown by a layer defined by geom_point to show the location of the variables at a point in time. The other type of glyph is lines to show the sequential trajectory, ordered by time, from one point to the next. This is captured by the layer geom_path. The figure below shows such a graph.\n\nggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path() +\n  geom_point()\n\n\n\n\nIt is hard to understand the direction of time from the lines alone. For example, it is difficult to visualize where the first, the last, or any years in between have happened.\nThis can be addressed by adding a mapping from the property colour to the variable year in the layer geom_point. R uses a default colour scale to assign specific colours from a colour palette to years.\nThe ggplot2 package defines the function aes() to create this many to many mapping.\n\nyear &lt;- function(x) as.POSIXlt(x)$year + 1900\nggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path(colour = \"grey50\") +\n  geom_point(aes(colour = year(date)))\n\n\n\n\nThe layer geom_path has a mapping from each line created between points to the same colour value indicated by the specification “grey50”. The syntax does not require the use of the aes() function. It is a many to one mapping.\nThis plot is a good attempt at representing the time dimension with a varying shade of colour. This solution is not entirely satisfactory because the lines get too entangled making the progress of time confusing in some quadrants."
  },
  {
    "objectID": "posts/welcome/index.html#animation-to-the-rescue",
    "href": "posts/welcome/index.html#animation-to-the-rescue",
    "title": "Animation to explain coupled time-series",
    "section": "Animation to the rescue",
    "text": "Animation to the rescue\nWe can get a more sophisticated visualization by using animation to explain how the two variables change simultaneously as time passes. In the following plot, the values of unemployment rate and median unemployment length in weeks are displayed for every year. By pressing the PLAY button, one sees the points for each year over the line trajectory, from beginning to end. One can use the slider to visualize the position of the variables for any given year.\n\nlibrary(plotly)\nyear &lt;- function(x) as.POSIXlt(x)$year + 1900\np &lt;- ggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path(colour = \"grey75\") +\n  geom_point(aes(colour = year(date), frame = year(date)))\n\nfig &lt;- ggplotly(p)\n\nfig &lt;- fig %&gt;% animation_opts(1000,\n                              easing = \"elastic\", \n                              redraw = FALSE )\nfig &lt;- fig %&gt;% animation_button(x = 0.05, xanchor = \"left\",\n                                y = 1.1, yanchor = \"top\")\nfig &lt;- fig %&gt;% animation_slider(currentvalue = list(prefix = \"YEAR \",\n                                                    font = list(color=\"red\")))\nfig\n\n\n\n\n\nFrom watching the motion of the annual data after pressing the Play button, one gets the sense that for the first 41 years the values of these two time series remained within the quadrant below the 15 week and to the left of 4% unemployment rate except for the years 1982 and 83. Then after 2009 the median unemployment length in weeks has increased over and above any value of the previous years in the USA according to this dataset.\nThe animation has achieved the introduction of a new dimension to represent the flow of time over the bi-dimensional plane representing the two time observed variables. In non-digital media the only alternative we would have is representing time progression with other dimensions like point color intensity or perhaps point diameter."
  },
  {
    "objectID": "posts/animationRescue/index.html",
    "href": "posts/animationRescue/index.html",
    "title": "Animation to explain coupled time-series",
    "section": "",
    "text": "I consider Edward E. Tufte one of the modern experts in data visualization. I attended one of his talks in Seattle in May 2019. All attendees walked away with a beautiful set of his most important books, while I left with the challenge of stuffing the set in my carry-on bag on the flight back to Calgary that night.\nAside from the pleasant experience and the wonderful books, I bring this up because Tufte introduces many clever and even beautiful examples of time series visualizations in his book “The Visual Display of Quantitative Information” ([1]). He claims that visual excellence is achieved when one can convey an idea to an observer without taking too long, using the least amount of ink and printed space. This implies cramming the most information possible with the least use of resources without obscuring meaning or confusing the main idea.\nOne of the time-series examples is a facet grid plot representing the evolution of the rate of unemployment and inflation in several countries, see the figure below. The term “facet grid” is used in ggplot2 lingo to refer to a table of plots comparing different subgroups within the data. This is not your traditional time series plot.\nRecently I was preparing a CalgaryR meetup presentation and reading the book by Hadley Wickham, the creator of ggplot2 ([2]). To my surprise, the last example introducing the concept of plot geoms, just before presenting facet grids, matched the idea used in the subplots of the example mentioned by Tufte. I realized then that I had found a use case for animation to improve this visualization in the era of digital media. Allow me to explain in this post."
  },
  {
    "objectID": "posts/animationRescue/index.html#the-problem-statement",
    "href": "posts/animationRescue/index.html#the-problem-statement",
    "title": "Animation to explain coupled time-series",
    "section": "The problem statement",
    "text": "The problem statement\nTo find a solution, let’s first highlight the problem. To do so we will formulate a research question and gradually build up a graphical representation to answer it. To do this, we will walk through the example from Hadley Wickham’s book on ggplot2 ([2]).\nWe want to show that by pushing the tools to their maximum we reach a climax, a point where we are constrained by the lack of motion. At that point in the story, we will have the right context to present animation to uncover a hidden feature of our data set.\nThe problem is how confusing it can be to add time as a third variable in a plot where each point is the intersection of two variables at different times.\nIt is simple to represent a time series by drawing the variable on one axis and time on the other. Tufte has many creative examples from several authors using this principle in his book. The difficulty becomes visualizing two of those time series on the same graph.\nA possible research question is what has been tried to address this limitation? And also, is there a novel approach to add visual dimensions to a flat X-Y plot without sacrificing visual excellence?"
  },
  {
    "objectID": "posts/animationRescue/index.html#the-economics-data-set",
    "href": "posts/animationRescue/index.html#the-economics-data-set",
    "title": "Animation to explain coupled time-series",
    "section": "The economics data set",
    "text": "The economics data set\nWith the problem statement out of the way, let’s move on to the .\nThese examples are re-used from section 2.6.5 of https://ggplot2-book.org/getting-started#sec-line.\nThe data set called economics from the ggplot2 package, has employment statistics on the US measured over the last 40 years up until 2015.\nHere is a brief look at the first 5 out of 574 rows of the dataframe economics.\n\ndata &lt;- head(economics, n=5)\nknitr::kable(data)\n\n\n\n\ndate\npce\npop\npsavert\nuempmed\nunemploy\n\n\n\n\n1967-07-01\n506.7\n198712\n12.6\n4.5\n2944\n\n\n1967-08-01\n509.8\n198911\n12.6\n4.7\n2945\n\n\n1967-09-01\n515.6\n199113\n11.9\n4.6\n2958\n\n\n1967-10-01\n512.2\n199311\n12.9\n4.9\n3143\n\n\n1967-11-01\n517.4\n199498\n12.8\n4.7\n3066"
  },
  {
    "objectID": "posts/animationRescue/index.html#visualizing-the-unemployment-rate",
    "href": "posts/animationRescue/index.html#visualizing-the-unemployment-rate",
    "title": "Animation to explain coupled time-series",
    "section": "Visualizing the Unemployment Rate",
    "text": "Visualizing the Unemployment Rate\nLet’s first make a simple time series plot of the unemployment rate. This is a continuous variable that is computed with the ratio unemploy/pop.\nIn ggplot2 a frame defines the first mapping from variables to a space where the data will be represented. It is created with the function aes(). The obvious frame for this plot is defined by the two variables date and unemploy / pop. They are mapped to the x and y coordinates of a 2-D plane. The glyphs drawn over this frame will be lines between the data points located in the frame, they are created with the function geom_line(). This function defines a layer over the frame.\n\nlibrary(plotly)\ng &lt;- ggplot(data = economics, mapping = aes(x = date, y = unemploy / pop)) +\n  geom_line()\ng &lt;- ggplotly(g)\ng\n\n\n\n\n\nTechnically speaking unemploy / pop represents the “population rate of unemployment as a fraction of the population able to work that is unemployed”, (https://www.bls.gov/cps/cps_htgm.htm#definitions)"
  },
  {
    "objectID": "posts/animationRescue/index.html#visualizing-the-unemployment-median-duration-in-weeks",
    "href": "posts/animationRescue/index.html#visualizing-the-unemployment-median-duration-in-weeks",
    "title": "Animation to explain coupled time-series",
    "section": "Visualizing the unemployment median duration in weeks",
    "text": "Visualizing the unemployment median duration in weeks\nAnother variable called uempmed from the same dataset tracks the median length of time in weeks of unemployment.\n\nlibrary(plotly)\ng &lt;- ggplot(economics, aes(date, uempmed)) +\n  geom_line()\ng &lt;- ggplotly(g)\ng\n\n\n\n\n\nFrom these two plots one can observe the recent trend towards longer median unemployment time in the decade of 2010. There are also cycles of between 5 and 10 years of peak unemployment rates.\nAn interesting question is how these two time series correlate over time. Are there interactions between these two variables that we could observe in one plot?"
  },
  {
    "objectID": "posts/animationRescue/index.html#visualizing-both-variables-in-the-same-plot",
    "href": "posts/animationRescue/index.html#visualizing-both-variables-in-the-same-plot",
    "title": "Animation to explain coupled time-series",
    "section": "Visualizing both variables in the same plot",
    "text": "Visualizing both variables in the same plot\nIn ggplot2, the frame for a representation that shows both variables on an line plot can be defined by a mapping of each variable to the x and y coordinates of the plane. We can create two types of glyphs over it: one is points shown by a layer defined by geom_point to show the location of the variables at a point in time. The other type of glyph is lines to show the sequential trajectory, ordered by time, from one point to the next. This is captured by the layer geom_path. The figure below shows such a graph.\n\nlibrary(plotly)\ng &lt;- ggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path() +\n  geom_point()\ng &lt;- ggplotly(g)\ng\n\n\n\n\n\nIt is hard to understand the direction of time from the lines alone. For example, it is difficult to visualize where the first, the last, or any years in between have happened.\nThis can be addressed by adding a mapping from the property colour to the variable year in the layer geom_point. R uses a default colour scale to assign specific colours from a colour palette to years.\nThe ggplot2 package defines the function aes() to create this many to many mapping.\n\nlibrary(plotly)\nyear &lt;- function(x) as.POSIXlt(x)$year + 1900\ng &lt;- ggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path(colour = \"grey50\") +\n  geom_point(aes(colour = year(date)))\ng &lt;- ggplotly(g)\ng\n\n\n\n\n\nThe layer geom_path has a mapping from each line created between points to the same colour value indicated by the specification “grey50”. The syntax does not require the use of the aes() function. It is a many to one mapping.\nThis plot is a good attempt at representing the time dimension with a varying shade of colour. This is unsatisfactory because the lines get too tangled, making the direction of time unclear. This looks eerily similar to the example Tufte refers to in his book, the difference is that we don’t print years near some of the dots. Instead we chose to use the shade of blue in the points as a way to convey information about the year."
  },
  {
    "objectID": "posts/animationRescue/index.html#animation-to-the-rescue",
    "href": "posts/animationRescue/index.html#animation-to-the-rescue",
    "title": "Animation to explain coupled time-series",
    "section": "Animation to the rescue",
    "text": "Animation to the rescue\nWe can get a more sophisticated visualization by using animation to explain how the two variables change simultaneously as time passes. In the following plot, the values of unemployment rate and median unemployment length in weeks are displayed for every year. By pressing the PLAY button, one sees the points for each year over the line trajectory, from beginning to end. One can use the slider to visualize the position of the variables for any given year.\n\nlibrary(plotly)\nyear &lt;- function(x) as.POSIXlt(x)$year + 1900\np &lt;- ggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path(colour = \"grey75\") +\n  geom_point(aes(colour = year(date), frame = year(date)))\n\nfig &lt;- ggplotly(p)\n\nfig &lt;- fig %&gt;% animation_opts(1000,\n                              easing = \"elastic\", \n                              redraw = FALSE )\nfig &lt;- fig %&gt;% animation_button(x = 0.05, xanchor = \"left\",\n                                y = 1.1, yanchor = \"top\")\nfig &lt;- fig %&gt;% animation_slider(currentvalue = list(prefix = \"YEAR \",\n                                                    font = list(color=\"red\")))\nfig\n\n\n\n\n\nFrom watching the motion of the annual data after pressing the Play button, one gets the sense that for the first 41 years the values of these two time series remained within the quadrant below the 15 week and to the left of 4% unemployment rate except for the years 1982 and 83. Then after 2009 the median unemployment length in weeks has increased over and above any value of the previous years in the USA according to this dataset.\nAt this point an economist might formulate the research question, what was the cause of this change in mean time to regain employment status? I would even suggest reviewing other available independent data sources, to confirm the existence of this trend.\nAs far as the exploratory work goes, this animation has achieved the introduction of a new dimension to represent the flow of time over the plane representing the two observed variables.\nBefore the digital era the only alternative we would have was representing time progression with other dimensions like point color intensity or perhaps point diameter.\n\nReferences\n\n\n[1] Tufte ER. The Visual Display of Quantitative Information. Second Ed. Graphics Press; 2001.\n\n\n[2] Wickham H. ggplot2: Elegant Graphics for Data Analysis. Second Ed. Springer-Verlag New York; 2016."
  },
  {
    "objectID": "posts/AnimationWorkflow /index.html",
    "href": "posts/AnimationWorkflow /index.html",
    "title": "How to create a GIF file from individual images",
    "section": "",
    "text": "Do you ever feel like a plain graph is not sufficient to explain some trend in your data or the solution of a mathematical model? For those times, generating animations can be just what you need. They can also become visualization aids during data exploration or a means to explain to yourself and others observed or simulated results.\nThis is by no means an exhaustive list of use cases, however, they are pretty common and their impact can be high. The deliverable is an embedded moving graph for your presentation or web content. After all, if an image is worth a thousand words then an animation should be worth a bit more.\nThis workflow is the basis to create the visualizations used in this LinkedIn post [1]\n\n\n\nAnimation from the post mentioned in [1]\n\n\n\n\n\n\n[1] Adames P. Expected Values and Variance 2021."
  },
  {
    "objectID": "posts/AnimationWorkflow /index.html#why-doing-this",
    "href": "posts/AnimationWorkflow /index.html#why-doing-this",
    "title": "How to create a GIF file from individual images",
    "section": "",
    "text": "Do you ever feel like a plain graph is not sufficient to explain some trend in your data or the solution of a mathematical model? For those times, generating animations can be just what you need. They can also become visualization aids during data exploration or a means to explain to yourself and others observed or simulated results.\nThis is by no means an exhaustive list of use cases, however, they are pretty common and their impact can be high. The deliverable is an embedded moving graph for your presentation or web content. After all, if an image is worth a thousand words then an animation should be worth a bit more.\nThis workflow is the basis to create the visualizations used in this LinkedIn post [1]\n\n\n\nAnimation from the post mentioned in [1]\n\n\n\n\n\n\n[1] Adames P. Expected Values and Variance 2021."
  },
  {
    "objectID": "posts/AnimationWorkflow/index.html",
    "href": "posts/AnimationWorkflow/index.html",
    "title": "How to create a GIF file from individual images",
    "section": "",
    "text": "Do you ever feel like a plain graph is not sufficient to explain some trend in your data or the solution of a mathematical model? For those times, generating animations can be just what you need. They can also become visualization aids during data exploration or a means to explain observations to yourself and others.\nThis is by no means an exhaustive list of use cases, however, they are pretty common and their impact can be high. The deliverable is an embedded moving graph for your presentation or web content. After all, if an image is worth a thousand words then an animation should be worth a bit more.\nThis workflow is the basis to create the visualizations used in this LinkedIn post [1]\n\n\n\nAnimation from the LinkedIn post referred to by [1]\n\n\nIn that post this animation was created using a simple but effective workflow.\n\nRun a numerical model using simulated input\nThe output of each run is used to construct a plot.\nEach plot is saved to a PNG file.\nThen a GIF file is constructed with all the jpeg files.\n\nTo illustrate this workflow we will use simulation. Other use cases may be sensor data or any observed data.\n\n\nWhen applying this algorithm to a simulation, the input in Step 1 above usually comes from a pseudo random number generator in your language of choice. In this case I used the following R code to generate the data before constructing the plots and the animation. The code computes the cumulative profit or loss from repeatedly playing the same wager for a very long time. Eventually the expected result should tend to zero for either wager 3 or 4.\n\nlibrary(lattice)\n\nnumsimulations = 20000\nnumframes = 30\n  \nset.seed(1892)\ndievalues = seq(1,6)\n\nfor (i in 1:numframes){\n  \n  die3 = sample(dievalues, numsimulations, replace=T)\n  die4 = sample(dievalues, numsimulations, replace=T)\n  \n  profit3 = 300*( die3&gt;3 ) - 300*( die3&lt;=3 )\n  profit4 = 150*( die4&gt;2 ) - 300*( die4&lt;=2 )\n  \n  runningprofit3 = cumsum(profit3) / seq(1, numsimulations)\n  runningprofit4 = cumsum(profit4) / seq(1, numsimulations)\n  \n  df3 &lt;- data.frame(wager=rep(3,length(runningprofit3)), runningprofit=runningprofit3, tries=seq(1,numsimulations))\n  df4 &lt;- data.frame(wager=rep(4,length(runningprofit4)), runningprofit=runningprofit4, tries=seq(1,numsimulations))\n  df &lt;- rbind(df3, df4)\n  \n  # more code follows here to generate the plots and the PNG files\n}\n\nLine 3 sets the number of simulation points to 20,000 so we get sharp plots. Line 5 sets the number of frames that the animation will have. Line 6 defines a fixed value for reproducible results from the pseudo random number generator used by the sample function that will be used in lines 11 and 12. Line 7 defines the 6 possible results of rolling a single die. Line 9 starts a loop to generate the frames of the animation. In lines 11 and 12 two vectors of 20,000 elements are created, each one simulates one roll of a fair die. The code in lines 14 and 15 create the two vectors of 20,000 elements, each represents the profit resulting from rolling the die once. Each throw is an independent event, thus their individual profit can be computed with the input vectors die3 and die4.\nLines 17 and 18 compute the cumulative profit or loss resulting from sequentially realizing the 20,000 wagers sequentially.\nThe following code inspects some of the vectors generated above for each frame. R is a natively vectorized language, therefore no special libraries are necessary. The library import statement in line 1 is for plot generation in a latter step.\n\nlength(runningprofit3)\n#&gt; [1] 20000\n\nhead(runningprofit3)\n#&gt; [1] -300    0 -100    0   60  100\ntail(runningprofit3)\n#&gt; [1] -2.925731 -2.910582 -2.895434 -2.880288 -2.895145 -2.880000\nhead(runningprofit4)\n#&gt; [1] -300.0  -75.0    0.0   37.5   60.0   75.0\ntail(runningprofit4)\n#&gt; [1] -1.822956 -1.815363 -1.830275 -1.845185 -1.860093 -1.852500\n\nOne can see how the initial profit or loss can be large, however the net value tends to be near zero after many repetitions.\n\n\n\nThe graphic package Trellis is designed to work on data presented in the so-called long format. The idea is that the dataframe with all the data has a column representing a categorical variable representing the subgroup the data belongs to. This is a fundamental idea behind this package. This plotting of many subgroups in a grid-like pattern is what is called faceting in ggplot2 terms.\n\n  df3 &lt;- data.frame(wager=rep(3,length(runningprofit3)), runningprofit=runningprofit3, tries=seq(1,numsimulations))\n  df4 &lt;- data.frame(wager=rep(4,length(runningprofit4)), runningprofit=runningprofit4, tries=seq(1,numsimulations))\n  df &lt;- rbind(df3, df4)\n\nIn lines 1 and 2 above, separate data frames each of 20,000 rows are created. In line 3 they are stacked vertically together to form a single data structure with 40,000 rows.\nThe following calls to the first and last three lines of the dataframe illustrate the long format of the data. The column wager represent the subgroup each row belongs to: wager 3 or 4.\n\nhead(df, n = 3)\n#&gt;   wager runningprofit tries\n#&gt; 1     3          -300     1\n#&gt; 2     3             0     2\n#&gt; 3     3          -100     3\ntail(df, n = 3)\n#&gt;       wager runningprofit tries\n#&gt; 39998     4     -1.845185 19998\n#&gt; 39999     4     -1.860093 19999\n#&gt; 40000     4     -1.852500 20000\n\nWe are ready to plot using the lattice plotting package. The following is the function that creates and returns such a lattice plot object. It receives two arguments, the dataframe in long form and a list of arguments to configure the plot.\n\n# Create a lattice xy plot object in memory ------------------------------------------------\n# Purpose:        to construct a valid lattice log_x-y plot of one or more series \n# Description:    The argument `the_data_frame` is in long form, where each row has data\n#                 for a sub-group. The list of arguments to construct the plot are:\n#                 1. the name of the variable that defines the subgroups, called subgroup_variable\n#                 2. a list of text labels for title, x and y axis, called text_labels_for_plot\n#                 3. a vector of the min and max values to use for the y axis, min_max_y_to_plot\n#                 4. a vector of text descriptions for every series to plot, or NULL, called subgroup_descriptions\ncreate_xyplot &lt;- function(the_data_frame, \n                          args_to_plot) {\n    # unpack arguments\n    the_subgroup_variable_name &lt;- args_to_plot[['subgroup_variable']]\n    the_text_plot_labels &lt;- args_to_plot[['text_labels_for_plot']]\n    min_max_y_to_plot &lt;- args_to_plot[['min_max_y_to_plot']]\n    the_subgroup_descriptions &lt;- args_to_plot[['subgroup_descriptions']]\n    \n    # setup values for plotting\n    group_labels &lt;- as.vector(unique(the_data_frame[the_subgroup_variable_name]))\n    wager_labels &lt;- as.vector(sapply( group_labels, FUN = function(x) {paste0(\"Wager \", x)}))\n    if (is.null(the_subgroup_descriptions)) {\n        the_subgroup_descriptions &lt;- wager_labels\n    }\n    wagers_vector &lt;- the_data_frame[[the_subgroup_variable_name]]\n\n    max_x_value &lt;- length(wagers_vector[wagers_vector==group_labels[[the_subgroup_variable_name]][1]])\n\n    xyplot(\n        runningprofit ~ tries,\n        data = the_data_frame,\n        groups = wager,\n        par.settings = list(superpose.line = list(\n            col = c(\"blue\", \"red\", \"green\", \"yellow\", \"brown\", \"cyan\"),\n            lwd = 1\n        )),\n        auto.key = list(\n            space = \"top\",\n            columns = 2,\n            text = the_subgroup_descriptions,\n            title = the_text_plot_labels[['title']],\n            cex.title = 2,\n            lines = TRUE,\n            points = FALSE\n        ),\n        xlab = the_text_plot_labels[['x_label']],\n        xlim = c(1, max_x_value),\n        scales = list(\n            cex = c(1.1, 1.1),            # increase font size\n            x = list(log = T),            # log scale for x-axis\n            y = list(log = F),\n            alternating = 1,              # axes labels left/bottom\n            tck = c(1, 0)\n        ),                                # ticks only with labels\n        ylab = the_text_plot_labels[['y_label']],\n        ylim = min_max_y_to_plot,\n        type = c(\"l\"),\n        panel = panel.superpose,\n        panel.groups = function(x, y, group.number, ...) {\n            panel.abline(h = y[which(y == 0.0)],\n                         lty = \"dotted\",\n                         col = \"black\")\n            panel.grid(v = -1,\n                       h = -1,\n                       lty = 3)\n            xt &lt;- x[x == log(min(x) + 1)] # find x coordinate for first point\n            yt &lt;- y[x == min(x)] # find y coordinate for first point\n            panel.text(xt,\n                       yt,\n                       labels = wager_labels[group.number],\n                       pos = 4,  # show labels on right side\n                       ...)\n            panel.xyplot(x, y, ...)\n        }\n    )\n}\n\nAn important detail of the lattice package is that it allows the superposition of the two subgroups: wager 3 and 4 thanks to the panel.superpose and the lambda function passed to the argument panel.groups. This panel type allows to paint the two different sub-groups on the same axis using the provided function. This function receives the group identifier when called once for each subgroup. In this particular case it adds programmatically a label to the first point of the series. The panel.superpose allows the positioning of the line plots corresponding to the two series on the same x-y grid so we can compare them visually.\n\n\n\nThe plot object generated in the previous step lives in the computer memory as an R object while we run the R session. We want to persist it in the file system as a PNG file. To achieve this goal we use the functions png and print.\nIn the code snipped below, I call the png function in line 1 to set the PNG file writer as the output device for subsequent calls to print images. Line ** 3** creates the arguments expected by the function as one list. The elements of the list are named so their order does not matter. Only the subgroup_descriptions can be NULL. Their function is explained by their name. In line 11 the lattice plot is created in memory and returned immediately to be assigned to the variable pp. The function xyplot comes from the package lattice. Line 14 passes the lattice object pp to the function print to create the PNG file using the name convention given in line 1. Multiple calls to print will create new PNG files adding one each timne to the numeric value represented by the %02d that is part of the file name.\n\npng(file=\"simwager%02d.png\", width=600, height=400)\n\nargs_to_plot &lt;- list( subgroup_variable = 'wager',\n                      subgroup_descriptions = c(\"Wager3 = 3(die&gt;3) - 3(die&lt;=3)\",\n                        \"Wager 4 = 1.5(die&gt;2) - 3(die&lt;=2)\"),\n                      text_labels_for_plot = list(title=\"Running profits from two different wagers\",\n                             x_label=\"Number of tries\",\n                             y_label=\"Cummulative profit\"),\n                      min_max_y_to_plot = c(-350, 350))\n\npp &lt;- create_xyplot(df, \n                    args_to_plot)\n\nprint(pp)\n\ndev.off()\n#&gt; png \n#&gt;   2\n\nThis code creates a file called simwager01.png in the current working directory for the R session, as shown below.\n\n\n\nPlot produced by the previous code chunk in the current working directory.\n\n\nIf this sequence of calls are part of an iteration loop, several PNG files will be created, each with an monotonically increased serial number as part of its name. For an explanation of how it works see the r-documentation for the function png\nI left some details out of the code shown at the beginning of this post because it would have been a distraction then. We must create a directory called animations from the project root path and then make it the current working directory as the first action in the script. By doing this all the file actions occur in the same folder. That is, the PNG files required for one animation are first created and then deleted after they are used in the same directory where the animated GIF file is created.\nThe following code accomplishes creating and changing into this directory.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\"here\")\n\nsuppressWarnings(my_proj_path &lt;- here())\n\nif (!file.exists(file.path(my_proj_path,\"animations\"))) {\n  dir.create(file.path(my_proj_path, \"animations\"))\n}\nsetwd(file.path(my_proj_path, \"animations\"))\n\n\n\n\nWhat we have up to this point is a directory with PNG files. The implicit assumption is that the order each PNG file is to be used to create the animation is implicit in its name. I will use a program to render an animated file in GIF format from the PNG files using the command line. The renderer used is convert from the package ImageMagik, publicly available as binaries for download for Linux, Mac and Windows, this example was run on Ubuntu Linux 24.04, version ImageMagick 6.9.12-98 Q16 x86_64 18038.\nTo call convert we make an operating system call from the R script. The arguments assume that all PNG files in the current directory are meant to be part of the GIF output. The order of the frames for the animation come from the digits included in the file names using the %02d in the call to the function png. Finally the PNG files are deleted from the folder.\n\nsystem(\"/usr/bin/convert -delay 40 *.png wager_comp_sim.gif\")\n\nfile.remove(list.files(pattern=\".png\"))\n\nThe delay argument sets the time pause between frames in centiseconds. This makes the frame rate approximately 2.5 frames per second.\nThe package gganimate, [2], works under the same workflow explained here but it gives the flexibility to choose renderer and the file format of the final animation."
  },
  {
    "objectID": "posts/AnimationWorkflow/index.html#why-doing-this",
    "href": "posts/AnimationWorkflow/index.html#why-doing-this",
    "title": "How to create a GIF file from individual images",
    "section": "",
    "text": "Do you ever feel like a plain graph is not sufficient to explain some trend in your data or the solution of a mathematical model? For those times, generating animations can be just what you need. They can also become visualization aids during data exploration or a means to explain observations to yourself and others.\nThis is by no means an exhaustive list of use cases, however, they are pretty common and their impact can be high. The deliverable is an embedded moving graph for your presentation or web content. After all, if an image is worth a thousand words then an animation should be worth a bit more.\nThis workflow is the basis to create the visualizations used in this LinkedIn post [1]\n\n\n\nAnimation from the LinkedIn post referred to by [1]\n\n\nIn that post this animation was created using a simple but effective workflow.\n\nRun a numerical model using simulated input\nThe output of each run is used to construct a plot.\nEach plot is saved to a PNG file.\nThen a GIF file is constructed with all the jpeg files.\n\nTo illustrate this workflow we will use simulation. Other use cases may be sensor data or any observed data.\n\n\nWhen applying this algorithm to a simulation, the input in Step 1 above usually comes from a pseudo random number generator in your language of choice. In this case I used the following R code to generate the data before constructing the plots and the animation. The code computes the cumulative profit or loss from repeatedly playing the same wager for a very long time. Eventually the expected result should tend to zero for either wager 3 or 4.\n\nlibrary(lattice)\n\nnumsimulations = 20000\nnumframes = 30\n  \nset.seed(1892)\ndievalues = seq(1,6)\n\nfor (i in 1:numframes){\n  \n  die3 = sample(dievalues, numsimulations, replace=T)\n  die4 = sample(dievalues, numsimulations, replace=T)\n  \n  profit3 = 300*( die3&gt;3 ) - 300*( die3&lt;=3 )\n  profit4 = 150*( die4&gt;2 ) - 300*( die4&lt;=2 )\n  \n  runningprofit3 = cumsum(profit3) / seq(1, numsimulations)\n  runningprofit4 = cumsum(profit4) / seq(1, numsimulations)\n  \n  df3 &lt;- data.frame(wager=rep(3,length(runningprofit3)), runningprofit=runningprofit3, tries=seq(1,numsimulations))\n  df4 &lt;- data.frame(wager=rep(4,length(runningprofit4)), runningprofit=runningprofit4, tries=seq(1,numsimulations))\n  df &lt;- rbind(df3, df4)\n  \n  # more code follows here to generate the plots and the PNG files\n}\n\nLine 3 sets the number of simulation points to 20,000 so we get sharp plots. Line 5 sets the number of frames that the animation will have. Line 6 defines a fixed value for reproducible results from the pseudo random number generator used by the sample function that will be used in lines 11 and 12. Line 7 defines the 6 possible results of rolling a single die. Line 9 starts a loop to generate the frames of the animation. In lines 11 and 12 two vectors of 20,000 elements are created, each one simulates one roll of a fair die. The code in lines 14 and 15 create the two vectors of 20,000 elements, each represents the profit resulting from rolling the die once. Each throw is an independent event, thus their individual profit can be computed with the input vectors die3 and die4.\nLines 17 and 18 compute the cumulative profit or loss resulting from sequentially realizing the 20,000 wagers sequentially.\nThe following code inspects some of the vectors generated above for each frame. R is a natively vectorized language, therefore no special libraries are necessary. The library import statement in line 1 is for plot generation in a latter step.\n\nlength(runningprofit3)\n#&gt; [1] 20000\n\nhead(runningprofit3)\n#&gt; [1] -300    0 -100    0   60  100\ntail(runningprofit3)\n#&gt; [1] -2.925731 -2.910582 -2.895434 -2.880288 -2.895145 -2.880000\nhead(runningprofit4)\n#&gt; [1] -300.0  -75.0    0.0   37.5   60.0   75.0\ntail(runningprofit4)\n#&gt; [1] -1.822956 -1.815363 -1.830275 -1.845185 -1.860093 -1.852500\n\nOne can see how the initial profit or loss can be large, however the net value tends to be near zero after many repetitions.\n\n\n\nThe graphic package Trellis is designed to work on data presented in the so-called long format. The idea is that the dataframe with all the data has a column representing a categorical variable representing the subgroup the data belongs to. This is a fundamental idea behind this package. This plotting of many subgroups in a grid-like pattern is what is called faceting in ggplot2 terms.\n\n  df3 &lt;- data.frame(wager=rep(3,length(runningprofit3)), runningprofit=runningprofit3, tries=seq(1,numsimulations))\n  df4 &lt;- data.frame(wager=rep(4,length(runningprofit4)), runningprofit=runningprofit4, tries=seq(1,numsimulations))\n  df &lt;- rbind(df3, df4)\n\nIn lines 1 and 2 above, separate data frames each of 20,000 rows are created. In line 3 they are stacked vertically together to form a single data structure with 40,000 rows.\nThe following calls to the first and last three lines of the dataframe illustrate the long format of the data. The column wager represent the subgroup each row belongs to: wager 3 or 4.\n\nhead(df, n = 3)\n#&gt;   wager runningprofit tries\n#&gt; 1     3          -300     1\n#&gt; 2     3             0     2\n#&gt; 3     3          -100     3\ntail(df, n = 3)\n#&gt;       wager runningprofit tries\n#&gt; 39998     4     -1.845185 19998\n#&gt; 39999     4     -1.860093 19999\n#&gt; 40000     4     -1.852500 20000\n\nWe are ready to plot using the lattice plotting package. The following is the function that creates and returns such a lattice plot object. It receives two arguments, the dataframe in long form and a list of arguments to configure the plot.\n\n# Create a lattice xy plot object in memory ------------------------------------------------\n# Purpose:        to construct a valid lattice log_x-y plot of one or more series \n# Description:    The argument `the_data_frame` is in long form, where each row has data\n#                 for a sub-group. The list of arguments to construct the plot are:\n#                 1. the name of the variable that defines the subgroups, called subgroup_variable\n#                 2. a list of text labels for title, x and y axis, called text_labels_for_plot\n#                 3. a vector of the min and max values to use for the y axis, min_max_y_to_plot\n#                 4. a vector of text descriptions for every series to plot, or NULL, called subgroup_descriptions\ncreate_xyplot &lt;- function(the_data_frame, \n                          args_to_plot) {\n    # unpack arguments\n    the_subgroup_variable_name &lt;- args_to_plot[['subgroup_variable']]\n    the_text_plot_labels &lt;- args_to_plot[['text_labels_for_plot']]\n    min_max_y_to_plot &lt;- args_to_plot[['min_max_y_to_plot']]\n    the_subgroup_descriptions &lt;- args_to_plot[['subgroup_descriptions']]\n    \n    # setup values for plotting\n    group_labels &lt;- as.vector(unique(the_data_frame[the_subgroup_variable_name]))\n    wager_labels &lt;- as.vector(sapply( group_labels, FUN = function(x) {paste0(\"Wager \", x)}))\n    if (is.null(the_subgroup_descriptions)) {\n        the_subgroup_descriptions &lt;- wager_labels\n    }\n    wagers_vector &lt;- the_data_frame[[the_subgroup_variable_name]]\n\n    max_x_value &lt;- length(wagers_vector[wagers_vector==group_labels[[the_subgroup_variable_name]][1]])\n\n    xyplot(\n        runningprofit ~ tries,\n        data = the_data_frame,\n        groups = wager,\n        par.settings = list(superpose.line = list(\n            col = c(\"blue\", \"red\", \"green\", \"yellow\", \"brown\", \"cyan\"),\n            lwd = 1\n        )),\n        auto.key = list(\n            space = \"top\",\n            columns = 2,\n            text = the_subgroup_descriptions,\n            title = the_text_plot_labels[['title']],\n            cex.title = 2,\n            lines = TRUE,\n            points = FALSE\n        ),\n        xlab = the_text_plot_labels[['x_label']],\n        xlim = c(1, max_x_value),\n        scales = list(\n            cex = c(1.1, 1.1),            # increase font size\n            x = list(log = T),            # log scale for x-axis\n            y = list(log = F),\n            alternating = 1,              # axes labels left/bottom\n            tck = c(1, 0)\n        ),                                # ticks only with labels\n        ylab = the_text_plot_labels[['y_label']],\n        ylim = min_max_y_to_plot,\n        type = c(\"l\"),\n        panel = panel.superpose,\n        panel.groups = function(x, y, group.number, ...) {\n            panel.abline(h = y[which(y == 0.0)],\n                         lty = \"dotted\",\n                         col = \"black\")\n            panel.grid(v = -1,\n                       h = -1,\n                       lty = 3)\n            xt &lt;- x[x == log(min(x) + 1)] # find x coordinate for first point\n            yt &lt;- y[x == min(x)] # find y coordinate for first point\n            panel.text(xt,\n                       yt,\n                       labels = wager_labels[group.number],\n                       pos = 4,  # show labels on right side\n                       ...)\n            panel.xyplot(x, y, ...)\n        }\n    )\n}\n\nAn important detail of the lattice package is that it allows the superposition of the two subgroups: wager 3 and 4 thanks to the panel.superpose and the lambda function passed to the argument panel.groups. This panel type allows to paint the two different sub-groups on the same axis using the provided function. This function receives the group identifier when called once for each subgroup. In this particular case it adds programmatically a label to the first point of the series. The panel.superpose allows the positioning of the line plots corresponding to the two series on the same x-y grid so we can compare them visually.\n\n\n\nThe plot object generated in the previous step lives in the computer memory as an R object while we run the R session. We want to persist it in the file system as a PNG file. To achieve this goal we use the functions png and print.\nIn the code snipped below, I call the png function in line 1 to set the PNG file writer as the output device for subsequent calls to print images. Line ** 3** creates the arguments expected by the function as one list. The elements of the list are named so their order does not matter. Only the subgroup_descriptions can be NULL. Their function is explained by their name. In line 11 the lattice plot is created in memory and returned immediately to be assigned to the variable pp. The function xyplot comes from the package lattice. Line 14 passes the lattice object pp to the function print to create the PNG file using the name convention given in line 1. Multiple calls to print will create new PNG files adding one each timne to the numeric value represented by the %02d that is part of the file name.\n\npng(file=\"simwager%02d.png\", width=600, height=400)\n\nargs_to_plot &lt;- list( subgroup_variable = 'wager',\n                      subgroup_descriptions = c(\"Wager3 = 3(die&gt;3) - 3(die&lt;=3)\",\n                        \"Wager 4 = 1.5(die&gt;2) - 3(die&lt;=2)\"),\n                      text_labels_for_plot = list(title=\"Running profits from two different wagers\",\n                             x_label=\"Number of tries\",\n                             y_label=\"Cummulative profit\"),\n                      min_max_y_to_plot = c(-350, 350))\n\npp &lt;- create_xyplot(df, \n                    args_to_plot)\n\nprint(pp)\n\ndev.off()\n#&gt; png \n#&gt;   2\n\nThis code creates a file called simwager01.png in the current working directory for the R session, as shown below.\n\n\n\nPlot produced by the previous code chunk in the current working directory.\n\n\nIf this sequence of calls are part of an iteration loop, several PNG files will be created, each with an monotonically increased serial number as part of its name. For an explanation of how it works see the r-documentation for the function png\nI left some details out of the code shown at the beginning of this post because it would have been a distraction then. We must create a directory called animations from the project root path and then make it the current working directory as the first action in the script. By doing this all the file actions occur in the same folder. That is, the PNG files required for one animation are first created and then deleted after they are used in the same directory where the animated GIF file is created.\nThe following code accomplishes creating and changing into this directory.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\"here\")\n\nsuppressWarnings(my_proj_path &lt;- here())\n\nif (!file.exists(file.path(my_proj_path,\"animations\"))) {\n  dir.create(file.path(my_proj_path, \"animations\"))\n}\nsetwd(file.path(my_proj_path, \"animations\"))\n\n\n\n\nWhat we have up to this point is a directory with PNG files. The implicit assumption is that the order each PNG file is to be used to create the animation is implicit in its name. I will use a program to render an animated file in GIF format from the PNG files using the command line. The renderer used is convert from the package ImageMagik, publicly available as binaries for download for Linux, Mac and Windows, this example was run on Ubuntu Linux 24.04, version ImageMagick 6.9.12-98 Q16 x86_64 18038.\nTo call convert we make an operating system call from the R script. The arguments assume that all PNG files in the current directory are meant to be part of the GIF output. The order of the frames for the animation come from the digits included in the file names using the %02d in the call to the function png. Finally the PNG files are deleted from the folder.\n\nsystem(\"/usr/bin/convert -delay 40 *.png wager_comp_sim.gif\")\n\nfile.remove(list.files(pattern=\".png\"))\n\nThe delay argument sets the time pause between frames in centiseconds. This makes the frame rate approximately 2.5 frames per second.\nThe package gganimate, [2], works under the same workflow explained here but it gives the flexibility to choose renderer and the file format of the final animation."
  },
  {
    "objectID": "posts/AnimationWorkflow/index.html#conclusion",
    "href": "posts/AnimationWorkflow/index.html#conclusion",
    "title": "How to create a GIF file from individual images",
    "section": "Conclusion",
    "text": "Conclusion\nThe basic workflow for the creation of a GIF animated file has been presented with luxury of detail. One has to consider four basic steps: organize the data whereas it is observed or simulated. Then create a meaningful plot that adresses the research question. Then create multiple PNG files and finally create the GIF file from the intermediary PNG files.\nThe complete code for this example can be found in the GitHub repo: wagers\n\nReferences\n\n\n[1] Adames P. Expected Values and Variance 2021.\n\n\n[2] Pedersen TL, Robinson D. Gganimate: A grammar of animated graphics. 2024."
  },
  {
    "objectID": "posts/StormData/index.html",
    "href": "posts/StormData/index.html",
    "title": "A User Interface to Analyze Storm Data",
    "section": "",
    "text": "Introduction\nThis work was done as the final project for the Coursera course Developing Data Products. Its purpose was to showcase the principles and tools studied in class by building a data product with data of interest to each one of the students. I have always being fascinated and also awed (terrified should I say) by the weather so I found the NOAA an appropriate source of publicly available information on storm events and their historical consequences.\nThe purpose of this app is data exploration. A more elaborate project would be to build a data model, transform the original data, and upload it to a proper relational database for consumption in a Shiny App. The UI would then allow querying the data in a more sophisticated way, for instance by filtering by the maximum, minimum, or a range of values of either fatalities or injuries or both for a type of disaster(s) within a time window.\nThe solution highlighted here was implemented by first transforming the original tabular data into a single R data frame that lives and is queried from memory.\nThis makes it a single tier application with everything running in a single host machine and a single process. The advantages are that it is easy to package and deploy, the disadvantage is that it is harder to update the data without application downtime.\n\n\nThe App Vs. The Data\nThe app can be seen online at https://padames.shinyapps.io/analysis_of_storm_data_from_naoo_data/\n\n\n\nA view of the App in a browser window.\n\n\nThe data was downloaded from ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/&gt;. Below a glimpse into the text format used originally.\n       STATE    EVTYPE FATALITIES INJURIES LATITUDE LONGITUDE\n55        AL      HAIL          0        0     3435      8700\n1456      AL HIGH WIND          0        0     3203      8613\n210826    MN LIGHTNING          0        0        0         0\n                                EVT_TYPE                DATE\n55                    Snow/Hail/Rain/Ice 1955-04-03 00:25:00\n1456   Hurricanes/Tornados/Thunderstorms 1975-04-01 00:10:00\n210826                   Fires/Lightning 1994-04-06 00:16:00\n\n\nThe Data\n\n902297 entries in the data set\nSpanning 5 decades\nSame seven categories for the storm events (as in previous work)\n\n\n\nTwo types of effects on population\n\n\n\nInstructions in the App\n\n\n\n\nGeolocating events and documenting the effects\nOne of the barriers to making sense of the data is understanding where and how often it happens.\n\n\n\nThe low zoom levels by default show the events aglomerated by region to avoid overcrowding the view\n\n\nThe effects can be seen in the image above in detail at the higher zoom levels (right pane with the map).\n\n\n\nHigher zoom level displays more granular data to the indivudual event as necessary\n\n\nR plus the Leaflet library and shiny offer a powerful visualization tool for complex data sets. It allows the user to select a subset of the data using three different criteria: type of the event, kind of consequence, and the time interval when any of the events happened.\nDoing the same in code would imply cleaning the raw data, transforming it into a suitable R data structure, subsetting to filter the events of interest and finally make a visualization to inspect the results. Indeed quite bit of wrangling with computer languages and iterative procedures.\n\n\nInsight into the data\nIt is easy to get distracted or fail to identify the trends in complex ata sets.\nThis data set is one such complex set but thanks to this App it can be explored to identify those features of the data that oftentimes remain hidden under the shear volume of the information.\nVisit https://padames.shinyapps.io/analysis_of_storm_data_from_naoo_data/\nTo download orignal data visit: ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/&gt;.\nThe format changed in 2014 making the data easier to understand and to process. However it’s still quite challenging to explore whitout an app."
  },
  {
    "objectID": "posts/StormData/StormData.html",
    "href": "posts/StormData/StormData.html",
    "title": "An User Interface to Analyze Storm Data",
    "section": "",
    "text": "An User Interface to Analyze Storm Data\nDate: April 2, 2017\nThis work was done as the final project for the Coursera course Developing Data Products.\n\n\nThe App Vs. The Data\nThe app can be seen online at https://padames.shinyapps.io/analysis_of_storm_data_from_naoo_data/\n\n\n\nA view of the App in a browser window.\n\n\nThe data was downloaded from ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/&gt;. Below a glimpse into the text format used originally.\n       STATE    EVTYPE FATALITIES INJURIES LATITUDE LONGITUDE\n55        AL      HAIL          0        0     3435      8700\n1456      AL HIGH WIND          0        0     3203      8613\n210826    MN LIGHTNING          0        0        0         0\n                                EVT_TYPE                DATE\n55                    Snow/Hail/Rain/Ice 1955-04-03 00:25:00\n1456   Hurricanes/Tornados/Thunderstorms 1975-04-01 00:10:00\n210826                   Fires/Lightning 1994-04-06 00:16:00\n\n\nThe Data\n\n902297 entries in the data set\nSpanning 5 decades\nSame seven categories for the storm events (as in previous work)\n\n\n\nTwo types of effects on population\n\n\n\nInstructions in the App\n\n\n\n\nGeolocating events and documenting the effects\nOne of the barriers to making sense of the data is understanding where and how often it happens.\n\n\n\nThe low zoom levels by default show the events aglomerated by region to avoid overcrowding the view\n\n\nThe effects can be seen in the image above in detail at the higher zoom levels (right pane with the map).\n\n\n\nHigher zoom level displays more granular data to the indivudual event as necessary\n\n\nR plus the Leaflet library and shiny offer a powerful visualization tool for complex data sets. It allows the user to select a subset of the data using three different criteria: type of the event, kind of consequence, and the time interval when any of the events happened.\nDoing the same in code would imply cleaning the raw data, transforming it into a suitable R data structure, subsetting to filter the events of interest and finally make a visualization to inspect the results. Indeed quite bit of wrangling with computer languages and iterative procedures.\n\n\nInsight into the data\nIt is easy to get distracted or fail to identify the trends in complex ata sets.\nThis data set is one such complex set but thanks to this App it can be explored to identify those features of the data that oftentimes remain hidden under the shear volume of the information.\nVisit https://padames.shinyapps.io/analysis_of_storm_data_from_naoo_data/\nTo download orignal data visit: ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/&gt;.\nThe format changed in 2014 making the data easier to understand and to process. However it’s still quite challenging to explore whitout an app."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#how-do-we-use-text",
    "href": "posts/HowToWriteAnyLanguage/index.html#how-do-we-use-text",
    "title": "How to process any text with Unicode",
    "section": "How do we use text?",
    "text": "How do we use text?\nThe human-readable text instructions of a computer program have to be interpreted and then transformed into machine-readable code before the computer can execute them. The first step implies parsing the text into functional parts: line breaks, plain data, commands, variable and function names and operators.\nThe next step is to put those components into and abstract syntax tree (AST) that represents the program. The AST is then further converted into bite code instructions for a run time or into machine instructions to be executed in a process under the supervision of the operating system scheduler, or even directly in the microchip of a single board computer in the case of embedded systems.\nEverybody benefits from this convention because it lowers the barrier to entry to computer programming as a discipline. It attempts to insulate the program as an artifact from the culture of the programmer. However, bias always favours the English-speaking programmer who knows how English reads.\nAccepting the historical facts, let’s dive into the ASCII character set. Its purpose is for programmers to encode rules that control the computer as it consumes input and produces output. We call these rules the program’s logic.\nThe input to and output from the program we will call data. Digital computers natively process only zeros and ones. Any computer data, including text, must be transformed into a sequence of these two values."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#conclusion",
    "href": "posts/HowToWriteAnyLanguage/index.html#conclusion",
    "title": "How to process any text with Unicode",
    "section": "Conclusion",
    "text": "Conclusion\nText representation using Unicode values and encondings has become an invisible technical detail. Its standardization, maintenance, and enhancement is in the hands of a few experts, interest groups and users that drive their constant adaptation to modern communication via the web.\nOf the computer languages tested, Rust, Python, and R have a rich supply of modules for accessing Unicode glyphs by subject, and also for parsing and rendering functions that apply the Unicode standards. The ICU libraries in C++ have a more nuanced API. Boost C++ has put a more modern API to the ICU libraries but it takes developer effort to become productive quickly.\nThe needs of the world have evolved in the last decades towards more internationalization through the Internet, global supply chain, and international e-commerce and trade. Rust and Python 3 have been developed in the last 15 years and their developer experience reflects their conscious decision to treat Unicode as a first class citizen and become popular languages for modern application development. Java was not tested here.\nAll the software we use handles text encodings, most likely UTF-8 or UTF-16 like Windows does, via the libraries of the computer languages or the operating system. We saw examples of how well they do it for the simplest of operations. I hope this has given the reader a feel for the size of the task when it comes to writing software that processes text as data in diverse languages. From human readable representation to binary data that can travel through wires, optic fibre, or electromagnetic waves, text is everywhere and it is not going away any time soon.\n\nReferences\n\n\n[1] Unicode Inc. Unicode technical standard #51 2024.\n\n\n[2] Wikipedia contributors. UTF-8 — Wikipedia, the free encyclopedia 2025.\n\n\n[3] The anonymous engineering team. Unicode converter – encode decode UTF text Base64 2018.\n\n\n[4] Unicode Inc. Unicode 16.0.0 Core Spec, unicode encoding forms 2025.\n\n\n[5] Unicode Inc. Unicode 16.0.0 Core Spec, 1.3 text handling 2025.\n\n\n[6] Unicode Inc. Emoji charts, v16.0 2025.\n\n\n[7] Emil Hvitfeldt, Hadley Wickham. Package emoji, v16.0.0.9000 2025."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html",
    "href": "posts/HowToWriteAnyLanguage/index.html",
    "title": "How to process any text with Unicode",
    "section": "",
    "text": "Chatbots, web pages, and stand-alone applications display information on the screens of our cell phones, home and office computers, at airports, bus and train stations. The text used in these scenarios enables clear communication. Text is not impeded by sound or obstructed by environmental noise and can deliver impact and meaning through semantically dense symbols reinforced by other visual or aural cues.\nText and written human language are, at their core, technologies developed to build and transmit culture. Accurate textual information is critical in international trade, e-commerce, and customer-facing software. It is safe to say that text has a fundamental role in sustaining our civilization.\n\n\n\nImage credit: https://neuroflash.com/blog/automatic-text-editor-online/"
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#text-is-everywhere",
    "href": "posts/HowToWriteAnyLanguage/index.html#text-is-everywhere",
    "title": "How to process any text with Unicode",
    "section": "",
    "text": "Chatbots, web pages, and stand-alone applications display information on the screens of our cell phones, home and office computers, at airports, bus and train stations. The text used in these scenarios enables clear communication. Text is not impeded by sound or obstructed by environmental noise and can deliver impact and meaning through semantically dense symbols reinforced by other visual or aural cues.\nText and written human language are, at their core, technologies developed to build and transmit culture. Accurate textual information is critical in international trade, e-commerce, and customer-facing software. It is safe to say that text has a fundamental role in sustaining our civilization.\n\n\n\nImage credit: https://neuroflash.com/blog/automatic-text-editor-online/"
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#what-is-text",
    "href": "posts/HowToWriteAnyLanguage/index.html#what-is-text",
    "title": "How to write in any language",
    "section": "What is text?",
    "text": "What is text?"
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#what-is-text-anyway",
    "href": "posts/HowToWriteAnyLanguage/index.html#what-is-text-anyway",
    "title": "How to process any text with Unicode",
    "section": "What is text anyway?",
    "text": "What is text anyway?\nTo narrow the scope of this post to a manageable size, let’s constrain the answer to the world of computer languages. These are used to tell computers what to do when they receive data.\nComputer programming starts by writing instructions in a text editor. The characters used to form those instructions were initially called the ASCII characters. These characters conveniently reflected the characters of the English alphabet plus the Arabic numerals and exclamation and punctuation marks.\nIn this context, text is used for two purposes. One is to write human-readable instructions for a computer to perform a task, the other is to be the data for the instructions to act upon."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#entering-text-encoding",
    "href": "posts/HowToWriteAnyLanguage/index.html#entering-text-encoding",
    "title": "How to process any text with Unicode",
    "section": "Entering text encoding",
    "text": "Entering text encoding\nRestraining myself to write about text only, I am ready to discuss the transformation of what we see as text in our preferred language into bits. A computer program must discern what part of those zeros and ones represent text and put together a message to act on it, whether it is displaying it on a screen, writing to a database, or “launching the missiles”, so to speak. Welcome to text decoding and encoding.\nThe complexity of this subject arises from the fact that human communication occurs within rich, dynamic, and diverse cultural contexts. Tradition, convention, identity, and art influence the glyphs and alphabets used to articulate words, sentences and, ultimately, ideas in human languages. Furthermore, each written language has rules for what a sentence and a word are, the direction of writing, what punctuation is, etc. It is easy to imagine that text can be as diverse as the human groups that use it.\nHow do we address this complexity? We use text encoding that machines can understand automatically and unequivocally. The accepted encoding standard is called Unicode. I will quote directly from the Unicode standard definition #51 [1].\n\nUnicode is the foundation for text in all modern software: it’s how all mobile phones, desktops, and other computers represent the text of every language.\n\nA good source of interesting information on Unicode can be found in the web page called the Unicode Primer. Only to give an idea of an interpretation of the 1,112,064 valid Unicode values or code points, have a squinting look at an image at of the Unifont character map. Just for context the ASCII characters fit in less than the first full row on this table.\n\n\n\nFigure 1. The Unifont full charcater map. Image Credit: https://free-images.com/or/8728/unifont_full_map_png.png\n\n\nA font table like the one above is needed to render the Unicode characters or glyphs from their numerical values. There are currently a total of 1,112,064 valid Unicode values or code points. These are scalar values represented in base-16 (using the digits and the letters A to F), their range of values can be seen in Table 1 below. The first 128 code points correspond to the original ASCII and yes you may try to go back to the Unifont table to find them on the first row.\nIndependent of the artistic design of the font, the Unicode values appear in official tables associated with the typographical or artistic rendition of the glyph they represent. Long before and after they are rendered to a screen or printer, each character is treated as a series of numbers.\nThe text encodings address the challenge of writing effectively in binary. Text consists of a sequence of characters, usually stored in contiguous memory. The encoding schema must also address the identification of character boundaries when all the program sees is endless zeros and ones.\n\nA Journey from text to bits and back\nTo illustrate the modern context in which we process text let’s consider a chat bot application in a mobile device, see the Figure 2 below. Text is entered on the mobile device through a virtual keyboard, rendered on screen, transformed ultimately into a byte stream with an UTF-8 encoding and sent through the wire/air using Internet protocols. On the receiving end, the bytes of zeros and ones are decoded into UTF-8, further transformed if necessary, interpreted using the indicated language and then displayed on the computer screen of the Browser used by the Customer Support agent to interact with the customer.\n\n\n\nFigure 2. The journey of a text message entered in a chat bot application in a mobile device to its destination in a computer screen of a customer support agent.\n\n\nText encoding is a mapping from the code points to some efficient way to write text in binary. A fixed bit width for each character would simplify parsing characters within some text sequence and it would make character boundaries trivial to infer, however, it would waste much space when representing the small ASCII characters. On the other side if we choose a variable bit width then space would be saved with the small values but an end-of-character marker would be required, also adding extra space and complexity.\nThe common Unicode text encoding to write source code in modern computer languages is called UTF-8, it is the encoding used to store the text of almost the entirety of the web pages of the Internet. UTF stands for the Unicode Transformation Format [2]. This is a super set of the ASCII character set used in the first computer languages like COBOL, FORTRAN and BASIC.\n\nTable 1. Code point to UTF-8 encoding, from [3]\n\n\n\n\n\n\n\n\n\nRange of Code Points\nByte 1\nByte 2\nByte 3\nByte 4\n\n\n\n\nFrom U+0000 to U007F\n0yyyzzzz\n\n\n\n\n\nFrom U+0080 to U+07FF\n110xxxyy\n10yyzzzz\n\n\n\n\nFrom U+0800 to U+FFFF\n1110wwww\n10xxxxyy\n10yyzzzz\n\n\n\nFrom U+010000 to U+10FFFF\n11110uvv\n10vvwwww\n10xxxxyy\n10yyzzzz\n\n\n\nThe code values or code points in Table 1 have a corresponding multi-byte representation in UTF-8 form, the encoding has a variable number of bytes. How many bytes is encoded by the number of consecutive ones in the most significant digits of the first byte in a byte sequence: none means the current byte is sufficient to encode the character fully, that is, it fits in one byte, e.g. an ASCII character. One means the byte is part of a multi-byte sequence. Two means the byte marks the start of a character encoded in two bytes in total. Three and four ones mean the equivalent, the byte marks the beginning of characters represented by three and four bytes respectively, including the leading one. Table 2 shows examples of each one of these cases.\n\nTable 2. Examples of UTF-8 bytes for characters in each of the ranges of Unicode points presented in Table 1.\n\n\nCharacter\nCode point\nByte 1\nByte 2\nByte 3\nByte 4\n\n\n\n\nH\nU+0048\n01001000\n\n\n\n\n\ná\nU+00E1\n11000011\n10100001\n\n\n\n\nᜋ\nU+170B\n11100001\n10011100\n10001011\n\n\n\n🍇\nU+01F347\n11110000\n10011111\n10001101\n10000111\n\n\n\nThis means that there is no need for escape sequences to mark the boundary of a single encoded multi-byte character. It also mean there is no need of a special marker for characters boundaries in a sequence of many characters. In addition to that it isolates semantic representation from character identification. Unicode encoding takes care of character boundaries but semantic parsing takes care of instruction or operator boundary."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#character-boundary-examples",
    "href": "posts/HowToWriteAnyLanguage/index.html#character-boundary-examples",
    "title": "How to write in any language",
    "section": "Character boundary examples",
    "text": "Character boundary examples\n\nC++\n\n\nRust\n\n\nPython\n\n\nR"
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#prqctical-character-boundary-examples",
    "href": "posts/HowToWriteAnyLanguage/index.html#prqctical-character-boundary-examples",
    "title": "How to write in any language",
    "section": "Prqctical character boundary examples",
    "text": "Prqctical character boundary examples\nI will present examples of character boundary identification in different computer languages. We will limit the scope to identifying emoticons mixed in with text for rendering text using UTF-8 encoding as most modern editors and terminals would do.\n\nC++\n\n\nRust\n\n\nPython\n\n\nR"
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#practical-character-boundary-examples",
    "href": "posts/HowToWriteAnyLanguage/index.html#practical-character-boundary-examples",
    "title": "How to process any text with Unicode",
    "section": "Practical character boundary examples",
    "text": "Practical character boundary examples\nI will present examples of character boundary identification in different computer languages. We will limit the scope to identifying emoticons mixed in with text for rendering text using UTF-8 encoding as most modern editors and terminals would do.\nI will be using emojis from the food-fruit category, as defined in the Unicode standard chart shown in the figure below.\n\n\n\nFigure 1. The table for the food-fruit emoji category taken from the Unicode standard chart [6]\n\n\n\nC++\nThe platform agnostic support for UTF-8 in C++ 20 is not quite as simple as in other modern computer languages. POSIX operating systems like Linux work with UTF-8 out of the box. However Windows uses UTF-16 internally, making every system call dependent on text encoding translations.\nThere is a C/C++ library for handling Unicode from the International Components for Unicode, ICU for short. This organization sits under the same umbrella of Unicode, Inc. The library is old judging by the 25 year-ago commits as of the time of writing. However, it is still maintained and it has been moved to GitHUb where it has a modern CI/CD pipeline.\nIn particular there is an ICU project that showcases the applications of the library through web pages for interactive text manipulation. The app for text segmentation is quite interesting. The app is hosted in icusegments-demo. The source code can be found in icu-demos in GitHub.\nI found that the pages are quite slow at certain times of day, using them is easier late at night in the North American time zones. Below is a successful test of the target sentence I want to parse by grapheme cluster, what we as users would identify as characters on the screen.\n\n\n\nFigure 2. The result from segmenting the test sentence using Unicode character boundaries using the web page at icusegment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the ICU library\nNow I am certain the library can be used for this task. The ICU library can be installed in an Ubuntu system either from the upstream repository through the package manager or by getting it from sources, then building and installing it locally. The last option is also available in a Windows operating system. Multiple versions can coexist simultaneously.\nAfter cloning the project from ICU, one can follow the steps in building-icu4c to configure, build and install the version 77.0.1 as of the date of this writing. A minimal working example was then build using CMake. The only caveat is to explicitly link the ICU shared libraries necessary, as an example, using the GNU toolchain on Linux: g++ your_file.cpp -o your_program -licuio -licui18n -licuuc, for a program that needs the libicuio.so, libicui18n.so, and libicuuc.so shared libraries.\n\n\nThe C++ test driver code\nAs far as the code goes, below is a first attempt at illustrating what analysis we want to do. We want to segment an UTF-8 string with variable width code units that represent a blend of plain English ASCII and some emojis.\nint main(int argc, char **argv)\n{\n    std::vector&lt;std::string&gt; fruit;\n    fruit.push_back(\"🍇\");\n    fruit.push_back(\"🍈\");\n    fruit.push_back(\"🍉\");\n    fruit.push_back(\"🍊\");\n    fruit.push_back(\"🍋\");\n\n    std::string test_string = fruit[0];\n\n    std::string long_fruity_string = \"Hello \" + test_string + \", \" + fruit[1] + \"!\";\n    \n    //Note: Since C++17 and ICU 76, you can use UTF-16 string literals with compile-time\n    //      length determination. \n    const icu::UnicodeString str = icu::UnicodeString::fromUTF8(long_fruity_string);\n    std::string st;\n    str.toUTF8String(st);\n    std::cout &lt;&lt; \"Sentence to parse: \\\"\" &lt;&lt; st &lt;&lt; \"\\\"\" &lt;&lt; std::endl;\n\n    UErrorCode err = U_ZERO_ERROR;\n    \n    std::unique_ptr&lt;icu::BreakIterator&gt; iter(\n        icu::BreakIterator::createCharacterInstance(icu::Locale::getDefault(), err));\n    assert(U_SUCCESS(err));\n    iter-&gt;setText(str);\n    auto start = iter-&gt;first();\n    auto end = start;\n    std::string charToPrintUTF8{};\n    auto ctp = icu::UnicodeString{};\n    while (iter-&gt;next() != icu::BreakIterator::DONE) {\n        start = end; // set start of the current grapheme before a pass throught the loop\n        end = iter-&gt;current(); // update the end position of the current grapheme boundary found via iter-&gt;next \n        auto len = end - start; // in code units containing a grapheme\n        charToPrintUTF8.clear(); \n        ctp = str.tempSubString(start, len); \n        ctp.toUTF8String(charToPrintUTF8);\n        auto grapheme = len&lt;2? std::string{charToPrintUTF8.front()}: charToPrintUTF8;\n        std::cout &lt;&lt; \"Code units: \" &lt;&lt; len &lt;&lt; \"; grapheme: '\" &lt;&lt; grapheme &lt;&lt; \"' of \" &lt;&lt; charToPrintUTF8.length() &lt;&lt; \" bytes\" &lt;&lt; std::endl;\n    }\n    return 0; \n}\nThe output is shown below when this code is compiled using the GNU compiler g++ 13.3 using the C++17 standard, the ICU libraries version 77.0.1 (built from latest GitHub as of this writing), CMake version 3.28.3 for the build system, and Ubuntu 24.04 for the Linux operating system. The output was edited to remove unnecessary details, linking to the ICU libraries is not a simple job, there is a learning curve to go through. The pay off for this complexity is a lot of control in the output.\n$ cmake ../ -DCMAKE_BUILD_TYPE=Debug\n-- The C compiler identification is GNU 13.3.0\n-- The CXX compiler identification is GNU 13.3.0\n...\n-- Found the following ICU libraries:\n--   data (required): /usr/local/lib/libicudata.so\n--   uc (required): /usr/local/lib/libicuuc.so\n--   i18n (required): /usr/local/lib/libicui18n.so\n--   io (required): /usr/local/lib/libicuio.so\n-- Found ICU: /usr/local/include (found suitable version \"77.0.1\", minimum required is \"77.0\") \n-- Configuring done (0.4s)\n-- Generating done (0.0s)\n-- Build files have been written to: /home/pablo/git/CPP/character_boundaries/build-debug\nThu Jan 30, 21:48:26; pablo@XPS13:~/git/CPP/character_boundaries/build-debug \n$ make\n[ 50%] Building CXX object CMakeFiles/cbd.dir/main.cpp.o\n[100%] Linking CXX executable cbd\n[100%] Built target cbd\nThu Jan 30, 22:58:20; pablo@XPS13:~/git/CPP/character_boundaries/build-debug [main]\n$ ./cbd\nSentence to parse: \"Hello 🍇, 🍈!\"\nCode units: 1; grapheme: 'H' of 1 bytes\nCode units: 1; grapheme: 'e' of 1 bytes\nCode units: 1; grapheme: 'l' of 1 bytes\nCode units: 1; grapheme: 'l' of 1 bytes\nCode units: 1; grapheme: 'o' of 1 bytes\nCode units: 1; grapheme: ' ' of 1 bytes\nCode units: 2; grapheme: '🍇' of 4 bytes\nCode units: 1; grapheme: ',' of 1 bytes\nCode units: 1; grapheme: ' ' of 1 bytes\nCode units: 2; grapheme: '🍈' of 4 bytes\nCode units: 1; grapheme: '!' of 1 bytes\nIt is non-trivial to find the character boundaries in C++. The ICU BreakIterator for characters and its functions next() and current() apply rules for grapheme cluster identification based on the locale and the Unicode pages for the glyphs. When UnicodeString object is parsed into these graphemes and an individual one is converted to UTF-8 we observe that the regular letters of the English alphabet, the white space and the punctuation characters use 1 byte, as expected, while the fruit emojis take 4 bytes, also as expected.\nThe reader is invited to look under the hood to the complexity in the parsing algorithm to identify these grapheme clusters. This kind of software is infrastructure that has to remain hidden so we can communicate using all of our languages and all of the emojis of the world in our devices.\nThe reader is also invited to look up this Stack Overflow answer on doing exactly the same character segmentation exercise in C++ using the Boost library and its module called locale. It has a more modern interface in terms of function names and paradigm. I like that it uses the term boost::locale::boundary::character as the function to map over an UTF-8 string using an object of type boost::locale::boundary::csegment_index. It is all in the naming!\n\n\nRust\nIn Rust I used the crate emojis from the official public registry. A crate is a module in Rust. For more details consult the free cargo online book.\nThe code creates a short vector with the first 5 elements of the Unicode emojis in the Food and Drink group. Then it creates a Rust String by concatenating String slices in UTF-8. It follows by using the magic of the String function chars to find the character boundaries and to print them one by one. The ‘magic’ comes from the fact that some of these characters have multiple code unit representations in UTF-8, the default this is the Unicode form used by the Rust String data type, regardless of the operating system.\nIn other words the function has to parse the byte sequence and using the UTF-8 decoding rules find whole characters, using the rules shown in Table 1. Once the base 16 Unicode code points are found a rendering function has to find them in a table and create the matching glyph on the terminal for printing.\nFinally, the binary sequence for the whole sentence is printed to the terminal.\nextern crate emojis;\n\nfn main() {\n    let fruit: Vec&lt;_&gt; = emojis::Group::FoodAndDrink.emojis().map(|e| e.as_str()).take(5).collect();\n    assert_eq!(fruit, [\"🍇\", \"🍈\", \"🍉\", \"🍊\", \"🍋\"]);\n    \n    let test_string = String::from(fruit[0]);\n    let long_fruity_string = \"Hello \".to_owned() + &test_string + \", \" + fruit[1] + \"!\"; \n    \n    println!(\"\\\"{}\\\"\", long_fruity_string);\n\n    for letter in long_fruity_string.chars() {\n        println!(\"{letter}\");\n    }\n    let mut long_fruity_string_in_binary = \"\".to_string();\n\n    for character in long_fruity_string.clone().into_bytes() {\n        long_fruity_string_in_binary += &format!(\"0{:b} \", character);\n    }\n    println!(r#\"\"{}\" in binary is \"{}\"\"#, long_fruity_string, long_fruity_string_in_binary);\n}\nThe output:\n$ cargo run\n   Compiling word_boundary v0.1.0 (/home/pablo/git/Rust/rust-practice/word_boundary)\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.24s\n     Running `target/debug/word_boundary`\n\"Hello 🍇, 🍈!\"\nH\ne\nl\nl\no\n \n🍇\n,\n \n🍈\n!\n\"Hello 🍇, 🍈!\" in binary is \"01001000 01100101 01101100 01101100 01101111 0100000 011110000 010011111 010001101 010000111 0101100 0100000 011110000 010011111 010001101 010001000 0100001 \"\nRemember the rules for representing UTF-8 encoded bytes from standard Unicode points, well if you have a look at the last four bytes of the sentence above: 11110000 10011111 10001101 10001000 removing the initial 0 that indicates binary numbers, we observe that these correspond to the 4 bytes of the multi-byte encoded character 🍈, U+1F348, in UTF-8 encoding in hexadecimal is f0 9f 8d 88 (from Table 3).\n\n\nPython\nComing soon…\n\n\nR\nR has the emoji package. After a short exploration of its manual [7] it is possible to print the characters of a sentence using the fruit emojis by identifying their boundaries boundaries seamlessly.\ninstall.packages(\"emoji\")\nlibrary(\"emoji\")\nfruit &lt;- emojis[ emojis$name %in% c(\"grapes\", \"watermelon\", \"melon\", \"lemon\", \"tangerine\"),]$emoji\ntest_string &lt;- fruit[1]\nlong_fruity_string &lt;- paste(\"Hello \", test_string,\", \",fruit[2], \"!\" )\npaste(long_fruity_string)\nresults &lt;- strsplit(x = long_fruity_string, split = \"\")\n#'strsplit' returns a list in case x has multiple strings to be processed, we grab only the first one\nresults[[1]]\nProduces the following output, a vector of characters, due to R’s native focus on vectorized operations:\n [1] \"Hello  🍇 ,  🍈 !\"\n [1] \"H\"  \"e\"  \"l\"  \"l\"  \"o\"  \" \"  \" \"  \"🍇\" \" \"  \",\"  \" \"  \" \"  \"🍈\" \" \"  \"!\""
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#how-do-we-use-text-as-code",
    "href": "posts/HowToWriteAnyLanguage/index.html#how-do-we-use-text-as-code",
    "title": "How to process any text with Unicode",
    "section": "How do we use text as code?",
    "text": "How do we use text as code?\nThe instructions that a computer program are made of have to be interpreted and then transformed into machine-readable code before the machine can execute them. The first step implies parsing the text into functional parts: line breaks, plain data, commands, operators, variable and function names.\nThe next step is to put those components into an abstract syntax tree (AST) that represents the program. The AST is then further converted into bite code instructions for a run time or into machine instructions to be executed by the operating system scheduler in a process under its supervision, or even directly in to the microchip clock scheduler of a single board computer in the case of real time embedded systems.\n\n\nThe input to and output from the program we call data. Digital computers natively process only zeros and ones. The text, images and sound supplied to a computer must be transformed first into a sequence of these two values.\nText as code is handled relatively well with a small character set. How about the handling of text, images, and sound as data?"
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-unicode-encodings",
    "href": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-unicode-encodings",
    "title": "How to process any text with Unicode",
    "section": "Trade-offs of Unicode encodings",
    "text": "Trade-offs of Unicode encodings\nUTF offers three forms to encode each of the 1,112,064 valid Unicode scalar values: UTF-8, UTF-16 and UTF-32. These Unicode values are also called code points, in base-16 they are the scalar values defined by the following two sequences: U+0000 to U+D7FF and U+E000 to U+10FFFF. The first 128 correspond to the original ASCII.\nEach encoding form maps the Unicode code points to unique code unit sequences [4]. In particular, the larger UNICODE code points may need multiple single code units if using the code form UTF-8. Conversely they may fit into a single code unit if encoded in the form UTF-32. This happens because the width of the smallest code unit is 8, 16, or 32 bits respectively for each of the forms.\nThe selection of a encoding form to map code points to code units forces a trade off between space and complexity. UTF-8 is very efficient for handling the smaller UNICODE values while it is complex to handle the higher ones, Conversely, UTF-32 is wasteful for storing the smaller values but simple for the higher ones.\nIn general, UTF-8 is the most suitable option for web pages and computer programs. The reason is only pragmatic because it makes the web engines and text editors and parsers work well when consuming most html, Javascript, and general purpose computer languages.\n\nMore definitions\nWe are not done with definitions yet. Some additional considerations are necessary to process text correctly, for this a detailed reading of the Unicode standard is advised [5]. I will focus first on the difference between characters and glyphs.\nA character is an abstract representation of a concrete mark made on paper or rendered to a computer screen, a so-called glyph. The character abstraction is expressed as a UNICODE value.\nThe standard defines how to represent and how to identify a text character as a code point, however it does not provide rules for determining what a valid text element is because that depends on what the context is. Examples of context are capitalization for a title in English text, or the brake down of long sequences of characters at the end of text lines.\nA text unit, thus is a valid sequence of one or more encoded text chraracters [5]."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-chosing-an-unicode-encoding",
    "href": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-chosing-an-unicode-encoding",
    "title": "How to process any text with Unicode",
    "section": "Trade-offs of chosing an Unicode encoding",
    "text": "Trade-offs of chosing an Unicode encoding\nUTF offers three forms to encode each of the 1,112,064 valid Unicode scalar values: UTF-8, UTF-16 and UTF-32. These Unicode values are also called code points, in base-16 they are the scalar values defined by the following two sequences: U+0000 to U+D7FF and U+E000 to U+10FFFF. The first 128 correspond to the original ASCII.\nEach encoding form maps the Unicode code points to unique code unit sequences [4]. In particular, the larger UNICODE code points may need multiple single code units if using the code form UTF-8. Conversely they may fit into a single code unit if encoded in the form UTF-32. This happens because the width of the smallest code unit is 8, 16, or 32 bits respectively for each of the forms.\nThe selection of a encoding form to map code points to code units forces a trade off between space and complexity. UTF-8 is very efficient for handling the smaller UNICODE values while it is complex to handle the higher ones, Conversely, UTF-32 is wasteful for storing the smaller values but simple for the higher ones.\nIn general, UTF-8 is the most suitable option for web pages and computer programs. The reason is only pragmatic because it makes the web engines and text editors and parsers work well when consuming most html, Javascript, and general purpose computer languages.\nThe Windows operating system uses UTF-16 for its internal encoding. As such Windows APIs must handle the UTF-8 to UTF-16 conversion internally.\n\nMore definitions\nWe are not done with definitions yet. Some additional considerations are necessary to process text correctly, for this a detailed reading of the Unicode standard is advised [5]. I will focus first on the difference between characters and glyphs.\nA character is an abstract representation of a concrete mark made on paper or rendered to a computer screen, a so-called glyph. The character abstraction is expressed as a UNICODE value.\nThe standard defines how to represent and how to identify a text character as a code point, however it does not provide rules for determining what a valid text element is because that depends on what the context is. Examples of context are capitalization for a title in English text, or the brake down of long sequences of characters at the end of text lines.\nA text unit, thus is a valid sequence of one or more encoded text chraracters [5]."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-chosing-a-unicode-encoding",
    "href": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-chosing-a-unicode-encoding",
    "title": "How to process any text with Unicode",
    "section": "Trade-offs of chosing a Unicode encoding",
    "text": "Trade-offs of chosing a Unicode encoding\nUTF offers three forms to encode each of the 1,112,064 valid Unicode scalar values: UTF-8, UTF-16 and UTF-32. These Unicode values are also called code points, in base-16 they are the scalar values defined by the following two sequences: U+0000 to U+D7FF and U+E000 to U+10FFFF. The first 128 correspond to the original ASCII.\nEach encoding form maps the Unicode code points to unique code unit sequences [4]. In particular, the larger UNICODE code points may need multiple single code units if using the code form UTF-8. Conversely they may fit into a single code unit if encoded in the form UTF-32. This happens because the width of the smallest code unit is 8, 16, or 32 bits respectively for each of the forms.\nThe selection of a encoding form to map code points to code units forces a trade off between space and complexity. UTF-8 is very efficient for handling the smaller UNICODE values while it is complex to handle the higher ones, Conversely, UTF-32 is wasteful for storing the smaller values but simple for the higher ones.\nIn general, UTF-8 is the most suitable option for web pages and computer programs. The reason is only pragmatic because it makes the web engines and text editors and parsers work well when consuming most html, Javascript, and general purpose computer languages.\nThe Windows operating system uses UTF-16 for its internal encoding. As such Windows APIs must handle the UTF-8 to UTF-16 conversion internally.\n\nMore definitions\nWe are not done with definitions yet. Some additional considerations are necessary to process text correctly, for this a detailed reading of the Unicode standard is advised [5]. I will focus first on the difference between characters and glyphs.\nA character is an abstract representation of a concrete mark made on paper or rendered to a computer screen, a so-called glyph. The character abstraction is expressed as a UNICODE value.\nThe standard defines how to represent and how to identify a text character as a code point, however it does not provide rules for determining what a valid text element is because that depends on what the context is. Examples of context are capitalization for a title in English text, or the brake down of long sequences of characters at the end of text lines.\nA text unit, thus is a valid sequence of one or more encoded text chraracters [5]."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-choosing-a-unicode-encoding",
    "href": "posts/HowToWriteAnyLanguage/index.html#trade-offs-of-choosing-a-unicode-encoding",
    "title": "How to process any text with Unicode",
    "section": "Trade-offs of choosing a Unicode encoding",
    "text": "Trade-offs of choosing a Unicode encoding\nThe code points can be encoded in three forms: UTF-8, UTF-16 and UTF-32. Each encoding maps the code points to unique code unit sequences of variable length [4]. In particular, the larger code points need multiple single code units if using the code form UTF-8. Conversely they may fit into a single code unit if encoded in UTF-32. This happens because the width of the smallest code unit is 8, 16, or 32 bits respectively for each of the forms. See Table 3 below for a comparison of three different encodings for a text sentence using short and long code points.\n\nTable 3. Unicode encoding of the sentence Hello 🍇, 🍈! from the website Convert Codes\n\n\nEncoding\nHexadecimal\n\n\n\n\nUTF-8\n48 65 6c 6c 6f 20 f0 9f 8d 87 2c 20 f0 9f 8d 88 21\n\n\nUTF-16\n0048 0065 006c 006c 006f 0020 d83c df47 002c 0020 d83c df48 0021\n\n\nUTF-32\n00000048 00000065 0000006c 0000006c 0000006f 00000020 0001f347 0000002c 00000020 0001f348 00000021\n\n\n\nThe long code points for the fruit emojis occupy a single UTF-32 code unit, look at 0001f347 and 0001f348, 🍇 and 🍈 in Table 4 below. from the same table one sees how these long codes require two code units in UTF-16 and four in UTF-8. That means more processing to read and write as more code units are required.\nThe selection of a encoding form to map code points to code units forces a trade off between space and complexity. UTF-8 is very efficient for handling the smaller Unicode values while it is complex to handle the large ones. Conversely, UTF-32 is wasteful for storing the smaller values but simple for the higher ones.\nWindows chose to use UTF-16 natively since the NT version. It is efficient for the Asian alphabets that require at least two code points in UTF-8 but one in UTF-16 for most of their characters.\nConsider the following emojis, their Unicode values and their multi-byte representation in three different encodings:\n\nTable 4. Food-fruit emoji codes and equivalent encoded representations\n\n\n\n\n\n\n\n\n\n\nfruit emoji\nname\nUnicode point\nUTF-8\nUTF-16\nUTF-32\n\n\n\n\n🍇\ngrapes\nU+1F347\nf0 9f 8d 87\nd83c udf47\n0001f347\n\n\n🍈\nmelon\nU+1F348\nf0 9f 8d 88\nd83c df48\n0001f348\n\n\n🍉\nwatermelon\nU+1F349\nf0 9f 8d 89\nd83c df49\n0001f349\n\n\n\nIn general, UTF-8 is the most suitable option for web pages and computer programs. The reason is only pragmatic because it makes the web engines and text editors and parsers work well when consuming most html, Javascript, and general purpose computer languages.\nThe Windows operating system uses APIs that handle the UTF-8 to UTF-16 conversion internally. In that way application code can pass UTF-8 encoded to the operating system.\n\nMore definitions\nWe are not done with definitions yet. Some additional considerations are necessary to process text correctly, for this a detailed reading of the Unicode standard is advised [5]. I will focus first on the difference between characters and glyphs.\nA character is an abstract representation of a concrete mark made on paper or rendered to a computer screen, a so-called glyph. The character is the Unicode value that matches a glyph by convention.\nThe standard defines how to represent and how to identify a text character as a code point, however it does not provide rules for determining what a valid text element is because that depends on what the context is. Examples of context are capitalization for a title in English text, or the brake down of long sequences of characters at the end of text lines.\nA text unit, thus is a valid sequence of one or more encoded text characters [5].\n\n\nAlgorithms for text encoding\nI will spare the reader with the specifics of these algorithms to go from code point to any of the encodings. A easy to follow example of these algorithms can be found elsewhere, I like the rich examples from this website: utf-8 encode-decode. If you follow those examples you will realize a text processing program needs a table of code points to glyphs, and an algorithm to encode code points to code units and the reversal."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#why-does-all-of-this-matters",
    "href": "posts/HowToWriteAnyLanguage/index.html#why-does-all-of-this-matters",
    "title": "How to process any text with Unicode",
    "section": "Why does all of this matters?",
    "text": "Why does all of this matters?\nText is used in the web and for any language, even for pseudo-languages like modern emojis. We expect web apps on mobile devices as well as web sites to be able to display any languaje and emoji correctly.\nThe programs that process text have to be break it down into its smallest components and then interpreted it as groups of characters that form words and punctuation, whole sentences, and whole paragraphs.\nThe main technique to accomplish this is called text boundary analysis. The ICU has an excellent explanation of it in its page on this subject."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#why-does-all-of-this-matter",
    "href": "posts/HowToWriteAnyLanguage/index.html#why-does-all-of-this-matter",
    "title": "How to process any text with Unicode",
    "section": "Why does all of this matter?",
    "text": "Why does all of this matter?\nText is used in the web and operating systems for any language, even for pseudo-languages like modern emojis. We have come to expect web apps, web sites, and our computers and mobile devices to be able to display our language and emojis correctly.\nThe programs that process text have to be break it down into its smallest components and then interpreted it as groups of characters that form words and punctuation, whole sentences, and whole paragraphs. And this has to work for any language supported by the software.\nI am avoiding the discussion on general rules for displaying numbers, dates, and so on, commonly included in the definition of the locale used by a computer. Keeping the focus exclusively on text, the main technique to process it is called text boundary analysis. In a nutshell, this is about finding where lines can be wrapped for display, where sentences end, how to move a cursor for one word to the next, how to find words and count them, how to move the cursor one character at a time, etc. The ICU has an excellent explanation of it in its page on this subject."
  },
  {
    "objectID": "posts/HowToWriteAnyLanguage/index.html#simple-character-boundary-examples",
    "href": "posts/HowToWriteAnyLanguage/index.html#simple-character-boundary-examples",
    "title": "How to process any text with Unicode",
    "section": "Simple character boundary examples",
    "text": "Simple character boundary examples\nI will present examples of character boundary identification in different computer languages. We will limit the scope to identifying emoticons mixed in with text for rendering text using UTF-8 encoding as most modern editors and terminals would do.\nI will be using emojis from the food-fruit category, as defined in the Unicode standard chart shown in the figure below.\n\n\n\nFigure 3. The table for the food-fruit emoji category taken from the Unicode standard chart [6]\n\n\n\nC++\nThe platform agnostic support for UTF-8 in C++ 20 is not quite as simple as in other modern computer languages. POSIX operating systems like Linux work with UTF-8 out of the box. However Windows uses UTF-16 internally, making every system call dependent on text encoding translations. Thankfully those translations are hidden under Windows API calls.\nThere is a C/C++ library for handling Unicode from the International Components for Unicode, ICU for short. This organization sits under the same umbrella of Unicode, Inc. The library is old judging by the 25 year-ago commits as of the time of writing. However, it is still maintained and it has been moved to GitHUb where it has a modern CI/CD pipeline. Furthermore the more modern Boost locale library uses the ICU libraries under the hood. These libraries are used by Linux packages everywhere to support internationalization.\nIn particular there is an ICU project that showcases the applications of the library through web pages for interactive text manipulation. The app for text segmentation is quite interesting. The app is hosted in icusegments-demo. The source code can be found in icu-demos in GitHub.\nI found that the pages are quite slow at certain times of day, using them is easier late at night in the North American time zones. Below is a successful test of the target sentence I want to parse by grapheme cluster, what we as users would identify as characters on the screen.\n\n\n\nFigure 2. The result from segmenting the test sentence using Unicode character boundaries using the web page at icusegment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the ICU library\nThe ICU library can be installed in a Ubuntu system either from the upstream repository through the package manager or by getting it from sources, then building and installing it locally. The last option is also available in a Windows operating system. Multiple versions can coexist simultaneously.\nAfter cloning the project from ICU, one can follow the steps in building-icu4c to configure, build and install the version 77.0.1 as of the date of this writing. A minimal working example was then build using CMake. The only caveat is to explicitly link the ICU shared libraries necessary, as an example, using the GNU toolchain on Linux: g++ your_file.cpp -o your_program -licuio -licui18n -licuuc, for a program that needs the libicuio.so, libicui18n.so, and libicuuc.so shared libraries.\n\n\nThe C++ test driver code\nAs far as the code goes, below is a first attempt at illustrating what analysis we want to do. We want to segment an UTF-8 string with variable width code units that represent a blend of plain English ASCII and some emojis.\nint main(int argc, char **argv)\n{\n    std::vector&lt;std::string&gt; fruit;\n    fruit.push_back(\"🍇\");\n    fruit.push_back(\"🍈\");\n    fruit.push_back(\"🍉\");\n    fruit.push_back(\"🍊\");\n    fruit.push_back(\"🍋\");\n\n    std::string test_string = fruit[0];\n\n    std::string long_fruity_string = \"Hello \" + test_string + \", \" + fruit[1] + \"!\";\n    \n    //Note: Since C++17 and ICU 76, you can use UTF-16 string literals with compile-time\n    //      length determination. \n    const icu::UnicodeString str = icu::UnicodeString::fromUTF8(long_fruity_string);\n    std::string st;\n    str.toUTF8String(st);\n    std::cout &lt;&lt; \"Sentence to parse: \\\"\" &lt;&lt; st &lt;&lt; \"\\\"\" &lt;&lt; std::endl;\n\n    UErrorCode err = U_ZERO_ERROR;\n    \n    std::unique_ptr&lt;icu::BreakIterator&gt; iter(\n        icu::BreakIterator::createCharacterInstance(icu::Locale::getDefault(), err));\n    assert(U_SUCCESS(err));\n    iter-&gt;setText(str);\n    auto start = iter-&gt;first();\n    auto end = start;\n    std::string charToPrintUTF8{};\n    auto ctp = icu::UnicodeString{};\n    while (iter-&gt;next() != icu::BreakIterator::DONE) {\n        start = end; // set start of the current grapheme before a pass throught the loop\n        end = iter-&gt;current(); // update the end position of the current grapheme boundary found via iter-&gt;next \n        auto len = end - start; // in code units containing a grapheme\n        charToPrintUTF8.clear(); \n        ctp = str.tempSubString(start, len); \n        ctp.toUTF8String(charToPrintUTF8);\n        auto grapheme = len&lt;2? std::string{charToPrintUTF8.front()}: charToPrintUTF8;\n        std::cout &lt;&lt; \"Code units: \" &lt;&lt; len &lt;&lt; \"; grapheme: '\" &lt;&lt; grapheme &lt;&lt; \"' of \" &lt;&lt; charToPrintUTF8.length() &lt;&lt; \" bytes\" &lt;&lt; std::endl;\n    }\n    return 0; \n}\nThe output is shown below when this code is compiled using the GNU compiler g++ 13.3 using the C++17 standard, the ICU libraries version 77.0.1 (built from latest GitHub as of this writing), CMake version 3.28.3 for the build system, and Ubuntu 24.04 for the Linux operating system. The output was edited to remove unnecessary details, linking to the ICU libraries is not a simple job, there is a learning curve to go through. The pay off for this complexity is a lot of control in the output.\n$ cmake ../ -DCMAKE_BUILD_TYPE=Debug\n-- The C compiler identification is GNU 13.3.0\n-- The CXX compiler identification is GNU 13.3.0\n...\n-- Found the following ICU libraries:\n--   data (required): /usr/local/lib/libicudata.so\n--   uc (required): /usr/local/lib/libicuuc.so\n--   i18n (required): /usr/local/lib/libicui18n.so\n--   io (required): /usr/local/lib/libicuio.so\n-- Found ICU: /usr/local/include (found suitable version \"77.0.1\", minimum required is \"77.0\") \n-- Configuring done (0.4s)\n-- Generating done (0.0s)\n-- Build files have been written to: /home/pablo/git/CPP/character_boundaries/build-debug\nThu Jan 30, 21:48:26; pablo@XPS13:~/git/CPP/character_boundaries/build-debug \n$ make\n[ 50%] Building CXX object CMakeFiles/cbd.dir/main.cpp.o\n[100%] Linking CXX executable cbd\n[100%] Built target cbd\nThu Jan 30, 22:58:20; pablo@XPS13:~/git/CPP/character_boundaries/build-debug [main]\n$ ./cbd\nSentence to parse: \"Hello 🍇, 🍈!\"\nCode units: 1; grapheme: 'H' of 1 bytes\nCode units: 1; grapheme: 'e' of 1 bytes\nCode units: 1; grapheme: 'l' of 1 bytes\nCode units: 1; grapheme: 'l' of 1 bytes\nCode units: 1; grapheme: 'o' of 1 bytes\nCode units: 1; grapheme: ' ' of 1 bytes\nCode units: 2; grapheme: '🍇' of 4 bytes\nCode units: 1; grapheme: ',' of 1 bytes\nCode units: 1; grapheme: ' ' of 1 bytes\nCode units: 2; grapheme: '🍈' of 4 bytes\nCode units: 1; grapheme: '!' of 1 bytes\nIt is non-trivial to find the character boundaries in C++. The ICU BreakIterator for characters and its functions next() and current() apply rules for grapheme cluster identification based on the locale and the Unicode pages for the glyphs. When UnicodeString object is parsed into these graphemes and an individual one is converted to UTF-8 we observe that the regular letters of the English alphabet, the white space and the punctuation characters use 1 byte, as expected, while the fruit emojis take 4 bytes, also as expected.\nThe reader is invited to look under the hood to the complexity in the parsing algorithm to identify these grapheme clusters. This kind of software is infrastructure that has to remain hidden so we can communicate using all of our languages and all of the emojis of the world in our devices.\nThe reader is also invited to look up this Stack Overflow answer on doing exactly the same character segmentation exercise in C++ using the Boost library and its module called locale. It has a more modern interface in terms of function names and paradigm. However, as I mentioned earlier, it relies on the ICU libraries to do the detailed work. I like it that it uses the term boost::locale::boundary::character as the function to map over an UTF-8 string using an object of type boost::locale::boundary::csegment_index. It is all in the naming!\nA caveat though, to be able to use Boost locale and the ICU libraries in the same executable their versions must match. The default Ubuntu 24.04 system Boost (v1.83) and ICU libraries (v74) did not match out out of the box because the Boost locale v1.83 was built with ICU v77. There is enough material for a full post on that subject alone.\n\n\nRust\nIn Rust I used the crate emojis from the official public registry. A crate is a module in Rust. For more details consult the free cargo online book.\nThe code creates a short vector with the first 5 elements of the Unicode emojis in the Food and Drink group. Then it creates a Rust String by concatenating String slices in UTF-8. It follows by using the magic of the String function chars to find the character boundaries and to print them one by one. The ‘magic’ comes from the fact that some of these characters have multiple code unit representations in UTF-8, the default this is the Unicode form used by the Rust String data type, regardless of the operating system.\nIn other words the function has to parse the byte sequence and using the UTF-8 decoding rules find whole characters, using the rules shown in Table 1. Once the base 16 Unicode code points are found a rendering function has to find them in a table and create the matching glyph on the terminal for printing.\nFinally, the binary sequence for the whole sentence is printed to the terminal.\nextern crate emojis;\n\nfn main() {\n    let fruit: Vec&lt;_&gt; = emojis::Group::FoodAndDrink.emojis().map(|e| e.as_str()).take(5).collect();\n    assert_eq!(fruit, [\"🍇\", \"🍈\", \"🍉\", \"🍊\", \"🍋\"]);\n    \n    let test_string = String::from(fruit[0]);\n    let long_fruity_string = \"Hello \".to_owned() + &test_string + \", \" + fruit[1] + \"!\"; \n    \n    println!(\"\\\"{}\\\"\", long_fruity_string);\n\n    for letter in long_fruity_string.chars() {\n        println!(\"{letter}\");\n    }\n    let mut long_fruity_string_in_binary = \"\".to_string();\n\n    for character in long_fruity_string.clone().into_bytes() {\n        long_fruity_string_in_binary += &format!(\"0{:b} \", character);\n    }\n    println!(r#\"\"{}\" in binary is \"{}\"\"#, long_fruity_string, long_fruity_string_in_binary);\n}\nThe output:\n$ cargo run\n   Compiling character_boundary v0.1.0 (/home/pablo/git/Rust/rust-practice/character_boundary)\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.41s\n     Running `target/debug/character_boundary`\n\"Hello 🍇, 🍈!\"\nH\ne\nl\nl\no\n \n🍇\n,\n \n🍈\n!\n\"Hello 🍇, 🍈!\" in binary is \"01001000 01100101 01101100 01101100 01101111 0100000 011110000 010011111 010001101 010000111 0101100 0100000 011110000 010011111 010001101 010001000 0100001 \"\nRemember the rules for representing UTF-8 encoded bytes from standard Unicode points, well if you have a look at the last four bytes of the sentence above: 11110000 10011111 10001101 10001000 removing the initial 0 that indicates binary numbers, we observe that these correspond to the 4 bytes of the multi-byte encoded character 🍈, U+1F348, in UTF-8 encoding in hexadecimal is f0 9f 8d 88 (from Table 3).\n\n\nPython\nPyhton 3 was a re-write of Python 2 in great part to make the language Unicode aware, therefore it is no surprise that it is one the most straight forward code for our character boundary discovery exercise.\n#!/usr/bin/env python3\n \ndef print_each_grapheme( unicode_string: str):\n    \"\"\" \n    prints each unicode grapheme cluster\n    \"\"\"\n    [print(c) for c in unicode_string]\n\n\nif __name__ ==\"__main__\":\n    \"\"\"\n    runs the script when called as a stand alone\n    \"\"\"\n    fruits = u\"🍇🍈🍉🍊🍋\"\n    test_string = fruits[0]\n    long_fruity_string = \"Hello \" + test_string + \", \" + fruits[1] +\"!\"\n    print_each_grapheme(long_fruity_string)\nPython 3 extends the concept of a string to an array of Unicode grapheme clusters. This facilitates character boundary identification using the idiomatic list comprehension syntax and a string iterator. Under the hood the iterator is using Unicode code pages. The output is as expected.\n$ ./main.py \nH\ne\nl\nl\no\n \n🍇\n,\n \n🍈\n!\n\n\nR\nR has the emoji package. After a short exploration of its manual [7] it is possible to print the characters of a sentence using the fruit emojis by identifying their boundaries boundaries seamlessly.\ninstall.packages(\"emoji\")\nlibrary(\"emoji\")\nfruit &lt;- emojis[ emojis$name %in% c(\"grapes\", \"watermelon\", \"melon\", \"lemon\", \"tangerine\"),]$emoji\ntest_string &lt;- fruit[1]\nlong_fruity_string &lt;- paste(\"Hello \", test_string,\", \",fruit[2], \"!\" )\npaste(long_fruity_string)\nresults &lt;- strsplit(x = long_fruity_string, split = \"\")\n#'strsplit' returns a list in case x has multiple strings to be processed, we grab only the first one\nresults[[1]]\nProduces the following output, a vector of characters, due to R’s native focus on vectorized operations:\n [1] \"Hello  🍇 ,  🍈 !\"\n [1] \"H\"  \"e\"  \"l\"  \"l\"  \"o\"  \" \"  \" \"  \"🍇\" \" \"  \",\"  \" \"  \" \"  \"🍈\" \" \"  \"!\"\n\n\nCharacter boundary identification complexity\nOne might be tempted to think that character boundary is always a simple task. By simple I mean, decode the bytes into Unicode code points, look them up in tables, render each. However some languages have special symbols to compose the individual glyphs into sentences, these have to be identified and decomposed. To illustrate this look at the python and Rust examples below.\nThe text comes from this Stack Overflow question cross-platform-iteration-of-unicode-string-counting-graphemes-using-icu\n#!/usr/bin/env python3\n\nfrom main import print_each_grapheme\n\nif __name__ == \"__main__\":\n    test_word = \"नमस्ते\"\n    print_each_grapheme(test_word)\nThe output is correct because the two combining marks are discovered correctly.\n\nRust does likewise:\nextern crate emojis;\n\nfn main() {\n    let tets_word = \"नमस्ते\";\n    println!(\"\\\"{}\\\"\", tets_word);\n\n    for letter in tets_word.chars() {\n        println!(\"{letter}\");\n    }\n}"
  }
]